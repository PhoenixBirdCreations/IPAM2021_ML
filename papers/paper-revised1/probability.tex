\subsection{Definition of Bayesian Probabilities} \label{bayesian_probs}

\tocheck{Let us define the probability of a candidate event $E$ being originated by a system with a \ac{NS} [$E(\hasns)=\true$] and \ac{EM} bright [$E(\hasrem)=\true$] given the classifier's outcome evaluated on the detection pipeline
output,} ${\bf A}_{\bf X}$, as $P(\hasns|{\bf A}_{\bf X})$ and $P(\hasrem|{\bf A}_{\bf X})$, respectively. The classifier outcome can be understood as a map $A:{\bf X}\to {\bf A}_{\bf X}$,
where \tocheck{${\bf X}(E)$} is a vector that identifies the output of the detection pipeline and ${\bf A}_{\bf X}$ is a vector that uniquely identifies the classifier algorithm's output for $\bf X$. 

Since a system can be \ac{EM} bright only when a \ac{NS} is present in the system, the condition $P(\hasrem|{\bf A}_{\bf X})\le P(\hasns|{\bf A}_{\bf X})$ must hold. However, if the
probabilities are calculated disjointly, this condition may be violated because of statistical and systematic errors in the pipeline's reconstructed signal parameters, as well as bias and
limited accuracy of the \ac{ML} algorithm. The approach discussed below avoids the occurrence of this inconsistency.

The \tocheck{true properties (ground truth)} of an observed event are unknown. Therefore, the probabilities $P(\hasns|{\bf A}_{\bf X})$ and $P(\hasrem|{\bf A}_{\bf X})$ cannot be calculated from observations. However,
estimators for $P(\hasns|{\bf A}_{\bf X})$ and $P(\hasrem|{\bf A}_{\bf X})$ can be calculated from synthetic events under the assumption that these events are a faithful representation of
real observations. \tocheck{This can be done as follows.}

According to Bayes' theorem, $P(\hasns|{\bf A}_{\bf X})$  can be written as 
%
\begin{equation}
P(\hasns|{\bf A}_{\bf X})=\frac{P({\bf A}_{\bf X}|\hasns)P(\hasns)}{P({\bf A}_{\bf X})}\,,
\label{bayes}
\end{equation}
%
where $P({\bf A}_{\bf X}|\hasns)$ is the likelihood of observing the classifier's outcome given an event with a \ac{NS} in the system, $P(\hasns)$ is the probability that a system includes
a \ac{NS}, and $P({\bf A}_{\bf X})$ is the probability of observing the classifier outcome ${\bf A}_{\bf X}$. \tocheck{Now consider a data set of synthetic events $E'$ defined as
$D=\{{\bf X}(E')\otimes{\bf L}(E')\}$, where ${\bf L}$ is a map to a vector space that assigns \hasns\ and \hasrem\ \emph{labels} given the event's \emph{properties}. We assume that the synthetic set
is a faithful representation of the space of possible real events, i.e., $E \simeq E'$. The probability $P(\hasns|A_{\bf X})$ in Eq.~\eqref{bayes} can be approximated as:} 
%
\begin{align}
\label{bayes-hasns}
P(\hasns|{\bf A}_{\bf X})&\simeq P(\hasns_+|{\bf A}_{{\bf X}'}) \nonumber \\
&=\frac{P({\bf A}_{{\bf X}'}|\hasns_+)P(\hasns_+)}{P({\bf A}_{{\bf X}'})},
\end{align}
%
where \tocheck{$\hasns_+=\{E'\,|\,E'(\hasns)=\true\}$ identifies the elements in $D$ with positive (+) \hasns\ true property, i.e., the subset of synthetic events that have
been simulated to contain a \ac{NS}, and ${\bf A}_{{\bf X}'}$ is the outcome of the classifier on ${\bf X}(E')$. The label $\hasns_+$ is determined by the values of the simulated
parameters of the events $E'$ before they are injected into the detection pipeline. Therefore, the classification label does not depend on the pipeline's outcome.} The probability $P(\hasrem|{\bf A}_{\bf X})$ can be approximated as:
%
\begin{align}
\label{bayes-hasrem}
P(&\hasrem|{\bf A}_{\bf X}) \nonumber  \\
&=P(\hasrem|\hasns, {\bf A}_{\bf X})P(\hasns|{\bf A}_{\bf X}) \nonumber \\
%&\simeq P(\hasrem_+|\hasns_+,A_{{\bf X}'})P(\hasns_+|A_{X'})\nonumber\\
&\simeq P(\hasrem_+|{\bf A}_{{\bf X}''})P(\hasns_+|{\bf A}_{{\bf X}'}),
\end{align}
%
where \tocheck{$\hasrem_+=\{E'\,|\,E'(\hasrem)=\true\}$ identifies the elements in $D$ with positive (+) \hasrem\ true property, i.e., the subset of synthetic events that have
been simulated to be \ac{EM}-bright, and ${\bf X}$ is the algorithm's outcome on the subset of events with property $E'(\hasns)=\true$,  ${\bf X}''= {\bf X}'(\hasns_+)$.}

To evaluate Eqs.~\eqref{bayes-hasns} and~\eqref{bayes-hasrem} with an \ac{ML} classifier, the synthetic data set is divided into two subsets, $D=D_R\oplus D_S$\tocheck{, where the $\oplus$ sign indicates complementary  subsets} . The $D_R$ subset is used
for algorithm training and validation. The $D_S$ subset is used to estimate the probabilities. Throughout this paper we use a 70\% -- 30\% split for $D_R$
and $D_S$, respectively \tocheck{\cite{split}}.

The choice of the labeling scheme and the algorithm's outcome depend on the \ac{ML} algorithm characteristics. Throughout this paper we implement a multi-label classification scheme,
where each element in $D$ is classified into $n$ mutually exclusive categories that uniquely define the $n$ possible physical states of the system. \tocheck{Given that we want to specify two probabilities for \hasns\ and \hasrem, we select the classifier outcome as a vector in a two-dimensional slice of the vector space ${\bf A}_{{\bf X}'}\subset \mathbb{R}^n$.} This scheme allows the calculation of Eqs.~\eqref{bayes-hasns} and \eqref{bayes-hasrem} with a single
training process. In the problem at hand, there are three possible physical states. A suitable labeling is
%
\tocheck{
\begin{align}
&{\bf L}[E'(\hasns)=\false]=0\,\nonumber\\
&{\bf L}[E'(\hasns=\true,\,\hasrem=\false)]=1\,\\
&{\bf L}[E'(\hasns=\true,\,\hasrem=\true)]=2\,.\nonumber
\end{align}
}
%
\tocheck{With the above definitions, $\hasrem_+$ is the set of events labeled ``2'' and $\hasns_+$ is the union of the sets labeled ``1'' and ``2''.} Therefore a natural choice for the algorithm outcome is
%
\begin{equation}
{\bf A}_{\bf X'}=(f_1+f_2,f_2)\subset (f_0,f_1,f_2)\,,
\end{equation}
%
where $f_0({\bf X}')=1-f_1({\bf X}')-f_2({\bf X}')$, $f_1({\bf X}')$, and $f_2({\bf X}')$ are the fractions of \ac{KNN} neighbors or \ac{RF} trees that predict the event to
have labels 0, 1, and 2, respectively.

The factors on the right-hand side of Eqs.~\eqref{bayes-hasns} and~\eqref{bayes-hasrem} can be obtained from $D_{S}$ once the algorithm has been trained on $D_{R}$. For example, for the \ac{KNN} and \ac{RF} scheme described earlier, the factors in Eq.~\eqref{bayes-hasns} can be estimated as  
%
\begin{align}
&P(\hasns_+)=\frac{N_{\hasns_+}}{N_s}\,,\nonumber\\
&P({\bf A}_{{\bf X}'}|\hasns_+)=\frac{N^+{}_{\hasns_+}(f_1+f_2)}{N_{\hasns_+}}\,,\\
&P({\bf A}_{{\bf X}'})=\frac{N^+{}_{\hasns_+}(f_1+f_2)+N^-{}_{\hasns_+}(f_1+f_2)}{N_s}\,,\nonumber
\label{bayes-est}
\end{align}
%
where \tocheck{$N_s$ is the number of events in $D_S$, $N_{\hasns_+}$ is the number of  $\hasns_+$ events in $D_S$, and $N^+{}_{\hasns_+}(f_1+f_2)$ and $N^-{}_{\hasns_+}(f_1+f_2)$ are the number of ${\hasns_+}$
events in $D_S$ that are correctly and incorrectly classified by the outcome $f_1+f_2$, respectively. The first factor on the right-hand side of Eq.~\eqref{bayes-hasrem}
can be evaluated similarly to Eq.~\eqref{bayes-hasns} by replacing \hasns\ with \hasrem\ and restricting $D_S$ to \hasns$_+$ elements.}

The probability estimators are generally noisy because they are evaluated on a finite data set. Smooth probability functions can be obtained by mapping them from the $(0,1)$ space to the
real line with a logistic function, smoothing them with a Savitzky-Golay filter, fitting them with \ac{GPR}, and finally mapping them back to the $(0,1)$ space.   

Both $P(\hasns|{\bf A}_{\bf X})$ and $P(\hasrem|{\bf A}_{\bf X})$ depend on the \ac{EOS} that is used to label the synthetic events. As the true \ac{EOS} of matter at \ac{NS} densities is
unknown, in order to minimize the systematics that arise in adopting a specific \ac{EOS} we consider a set of 23 different \ac{EOS} and marginalize Eqs.~\eqref{bayes-hasns} and
\eqref{bayes-hasrem} over them. The marginalized probabilities $P_M(\hasns|{\bf A}_{\bf X})$ and $P_M(\hasrem|{\bf A}_{\bf X})$ are defined as
%
\begin{equation}
\begin{aligned}
P_M(I|{\bf A}_{\bf X})=\frac{\sum_J\beta_J P_J(I|{\bf A}_{\bf X})}{\sum_J\beta_J}\,,
\label{bayes-marginalized}
\end{aligned}
\end{equation}
%
where $I=$ \hasns\ or \hasrem, $P_J(I|{\bf A}_{\bf X})$ ($J=1,\dots 23$) are the Bayesian probabilities in Eqs.~\eqref{bayes-hasns} and \eqref{bayes-hasrem} calculated from the data set
$D_S$ with labels assigned according to the $J$-th \ac{EOS}, and $\beta_J$ are Bayes' factors \tocheck{from Table~II (third column) of Ref.~\cite{Ghosh:2021eqv}}. The probabilities $P_J(I|{\bf A}_{\bf X})$ in
Eq.~\eqref{bayes-marginalized} can be tabulated and used to compute the marginalized probabilities $P_M(\hasns|{\bf A}_{\bf E})$ and $P_M(\hasrem|{\bf A}_{\bf E})$ for any new event $\bf
E$ with algorithm outcome $\bf A_{E}$. \tocheck{It is important to point out that this method does not depend on the specific value of the Bayes' factors.}
