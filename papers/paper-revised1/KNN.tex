\subsection*{K-Nearest Neighbors}

K-Nearest Neighbors is a non-parametric, supervised algorithm~\cite{Fix:1951,Cover:1967} that uses the fact that similar points in a data set are ``near'' each other in their parameter
space. When it is applied to classification problems~\cite{Guo:2004}, the algorithm is usually renamed \ac{KNN}. The algorithm captures the idea of similarity between points by computing
the distance between each point in a training set and its neighbors according to a pre-determined metric. Next, it sorts the neighbors in ascending order based on their distance to the
testing point. By choosing the top $K$ neighbors from the sorted array, \ac{KNN} assigns the label to the testing point that corresponds to the most frequent neighbor.

In this work, we use the open-source Python \ac{KNN} implementation of scikit-learn~\cite{Pedregosa:2011ork}. We fix the algorithm hyperparameters by cross-validating over the data set
and obtain the highest accuracy. Throughout our analysis we use $K = 8$ neighbors, the Manhattan metric, the BallTree algorithm and the neighbors are weighted by the inverse of their
distance to the event. \tocheck{Further details on hyperparameter tuning are given in Appendix~\ref{app:crossval}.} This configuration differs from the \ac{LVK}'s current implementation, which employs the Mahalanobis metric and $K = 2n + 1 = 11$ neighbors, where $n$ is the number
of features. Our configuration is the optimal choice for the new labeling scheme that is presented in Sect.~\ref{labelingscheme}).


