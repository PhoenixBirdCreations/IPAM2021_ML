\subsection*{Definition of Event Probabilities}

\label{sec:bayesian_probs}

Let us define the probability of a system having a \ac{NS} (\hasns:\true) and being \ac{EM} bright (\hasrem:\true), given an event $X$ and the outcome of a classifier on its detection pipeline
output, ${\bf A}_{\bf X}$, as $P(\hasns|{\bf A}_{\bf X})$ and $P(\hasrem|{\bf A}_{\bf X})$, respectively. The classifier outcome can be understood as a map $A:{\bf X}\to {\bf A}_{\bf X}$, where
$\bf X$ is a vector that identifies the output of the detection pipeline and ${\bf A}_{\bf X}$ is a vector that uniquely identifies the algorithm's output on $\bf X$. 

Since a system can be EM-bright only when a \ac{NS} is present in the system, the condition $P(\hasrem|{\bf A}_{\bf X})\subset P(\hasns|{\bf A}_{\bf X})$ must hold. However, if the probabilities
are calculated disjointly, as in the current \ac{LVK} implementation, this condition may be violated because of statistical and systematic errors in the pipeline's reconstructed signal
parameters, as well as bias and limited accuracy of the \ac{ML} algorithm. The approach discussed below avoids the occurrence of this inconsistency.

The ground truth of observed events is unknown. Therefore, the probabilities $P(\hasns|{\bf A}_{\bf X})$ and $P(\hasrem|{\bf A}_{\bf X})$ cannot be calculated from observations. However,
estimators for $P(\hasns|{\bf A}_{\bf X})$ and $P(\hasrem|{\bf A}_{\bf X})$ can be calculated from synthetic events under the assumption that these events are a faithful representation of
real observations.

Consider a data set of $N$ simulated events ${\bf X}'_i$, $D=\{{\bf X}'_i\otimes{\bf L}({\bf X}'_i)\}$, where $i=1\dots N$ and ${\bf L}$ is a map from ${\bf X}'$ to a vector space
that defines labels for the \hasns\ and \hasrem\ ground truths. According to Bayes theorem, $P(\hasns|{\bf A}_{\bf X})$  can be written as 
%
\begin{equation}
P(\hasns|{\bf A}_{\bf X})=\frac{P({\bf A}_{\bf X}|\hasns)P(\hasns)}{P({\bf A}_{\bf X})}\,,
\label{bayes}
\end{equation}
%
where $P({\bf A}_{\bf X}|\hasns)$ is the likelihood of observing the classifier's outcome given an event with a \ac{NS} in the system, $P(\hasns)$ is the probability that a system includes a \ac{NS}, and $P({\bf A}_{\bf X})$ is the probability of observing the classifier outcome ${\bf A}_{\bf X}$. The probability $P(\hasns|A_{\bf X})$ can be approximated as:  
%
\begin{align}
P(\hasns|{\bf A}_{\bf X})&\simeq P(\hasns_+|{\bf A}_{{\bf X}'})\nonumber\\
&=\frac{P({\bf A}_{{\bf X}'}|\hasns_+)P(\hasns_+)}{P({\bf A}_{{\bf X}'})}\,,
\label{bayes-hasns}
\end{align}
%
where $\hasns_+={\bf L}[{\bf X}'(\hbox{\hasns:\true})]$ is a label that uniquely identifies elements in $D$ with positive \hasns\ ground truth, ${\bf X}'$ is an element in $D$, and ${\bf A}_{{\bf X}'}$ is the outcome of the classifier on ${\bf X}'$. The probability $P(\hasrem|{\bf A}_{\bf X})$ can be approximated as:
%
\begin{align}
P(&\hasrem|{\bf A}_{\bf X})\nonumber\\
&=P(\hasrem|\hasns, {\bf A}_{\bf X})P(\hasns|{\bf A}_{\bf X})\nonumber\\
%&\simeq P(\hasrem_+|\hasns_+,A_{{\bf X}'})P(\hasns_+|A_{X'})\nonumber\\
&\simeq P(\hasrem_+|{\bf A}_{{\bf X}''})P(\hasns_+|{\bf A}_{{\bf X}'})
\label{bayes-hasrem}
\end{align}
%
where $\hasrem_+={\bf L}[{\bf X}'(\hbox{\hasrem:\true})]$ is a label that uniquely identifies elements in $D$ with positive \hasrem\ ground truth and ${\bf X}''= {\bf X}'(\hbox{\hasns:\true})$.

To evaluate Eqs.~(\ref{bayes-hasns}) and (\ref{bayes-hasrem}) with an \ac{ML} classifier, the synthetic data set is divided into two subsets, $D=D_R\oplus D_S$. The $D_R$ subset is used for
algorithm training and validation. The $D_S$ subset is used to estimate the probabilities. Following customary practice, throughout this paper we use a 70\%--30\% split for $D_R$ and $D_S$,
respectively.

The choice of the labeling scheme and the algorithm's outcome depend on the \ac{ML} algorithm characteristics. Throughout this paper we implement a multilabel classification scheme, where each
element in $D$ is classified into $n$ mutually exclusive categories that uniquely define the $n$ possible physical states of the system. The classifier outcome can then be chosen as a
two-dimensional vector space ${\bf A}_{{\bf X}'}\subset \mathbb{R}^n$. This scheme allows the calculation of Eqs.~(\ref{bayes-hasns}) and (\ref{bayes-hasrem}) with a single training process. In
the problem at hand, there are three possible physical states and a suitable labeling
map is 
%
\begin{align}
&{\bf L}[{\bf X}'(\hbox{\hasns:\false})]=0\,\nonumber\\
&{\bf L}[{\bf X}'(\hbox{\hasns:\true},\hbox{\hasrem:\false})]=1\,\\
&{\bf L}[{\bf X}'(\hbox{\hasns:\true},\hbox{\hasrem:\true})]=2\,.\nonumber
\end{align}
%
With this labeling, \hasrem$_+=2$ and \hasns$_+=1\cup 2$. Therefore a natural choice for the algorithm outcome is
%
\begin{equation}
{\bf A}_{\bf X'}=(f_1+f_2,f_2)\subset (f_0,f_1,f_2)\,,
\end{equation}
%
where $f_0({\bf X}')=1-f_1({\bf X}')-f_2({\bf X}')$, $f_1({\bf X}')$, and $f_2({\bf X}')$ are the fractions of \ac{KNN} neighbors or \ac{RF} trees that predict the event to
have labels 0, 1, and 2, respectively.

%The preferred scheme for binary classification algorithms that cannot efficiently handle multilabel classifications, such as \ac{GP}, is to separately label the events as  ${\bf L}({\bf
%X}'_i)={\bf L}_\hasns({\bf X}'_i)\otimes {\bf L}_\hasrem({\bf X}'_i)$ and separately train for \hasns\ and \hasrem. In this scheme $\hasns_+={\bf L}_\hasns[{\bf X}'(\hasns=\true)]$ and $\hasrem_+={\bf L}_\hasrem[{\bf X}'(\hasrem=\true)]$. As for the previous scheme, the physical condition $P(\hasrem|{\bf A}_{\bf X})\subset P(\hasns|{\bf A}_{\bf X})$ is ensured by the conditional probability definition for $P(\hasrem|{\bf A}_{\bf X})$ in Eq.~(\ref{bayes-hasrem}).
%
%~\\\todo{Do we need the table?}\\
%
%\begin{table}[h]
%\centering
%\begin{tabular}{@{}ccc@{}}
%\toprule
%Class:~\hasns & Class:~\hasrem & Label \\ \midrule
%\false     & \false      & 0         \\
%\true     & \false      & 1         \\
%\true     & \true      & 2         \\ \bottomrule
%\end{tabular}
%\caption{\tocheck{Labeling adopted for classification of having a NS and having a remnant with the same classifier}}
%\label{tab:multilabels}
%\end{table}

The factors in the right-hand side of Eqs.~(\ref{bayes-hasns}) and (\ref{bayes-hasrem}) can be obtained from $D_{S}$ once the algorithm has been trained on $D_{R}$. For example, for the \ac{KNN} and \ac{RF} scheme described earlier, the factors in Eq.~(\ref{bayes-hasns}) can be estimated as  
%
\begin{align}
&P(\hasns_+)=\frac{N_{\hasns_+}}{N_s}\,,\nonumber\\
&P({\bf A}_{{\bf X}'}|\hasns_+)=\frac{N^+{}_{\hasns_+}(f_1+f_2)}{N_{\hasns_+}}\,,\nonumber\\
&P({\bf A}_{{\bf X}'})=\frac{N^+{}_{\hasns_+}(f_1+f_2)+N^-{}_{\hasns_+}(f_1+f_2)}{N_s}\,,\label{bayes-est}
\end{align}
%
where $N_s=\hbox{dim}\,(D_S)$, $N_{\hasns_+}$ is the number of events in $D_S$ with label $\hasns_+$, and $N^+{}_{\hasns_+}(f_1+f_2)$ and $N^-{}_{\hasns_+}(f_1+f_2)$ are the number of events in $D_S$ that are correctly (incorrectly) classified as ${\hasns_+}$ by the outcome $f_1+f_2$. The first factor in the right-hand side of Eq.~(\ref{bayes-hasrem}) can be evaluated similarly to Eq.~(\ref{bayes-est}) by replacing \hasns\ with \hasrem\ and restricting $D_S$ to elements with \hasns$_+$ label.

The probability estimators are generally noisy because they are evaluated on a finite data set. Smooth probability functions can be obtained by mapping them from the $(0,1)$ space to the real
line with a logistic function, smoothing them with a Savitzky-Golay filter, fitting them with a Gaussian Process Regression, and finally mapping them back to the $(0,1)$ space.   

Both $P(\hasns|{\bf A}_{\bf X})$ and $P(\hasrem|{\bf A}_{\bf X})$ depend on the \ac{EOS} that is used to label the synthetic events. \todo{we need a bit more here, for example what difference soft EOS and hard EOS produce on the max NS mass, existence of remnant, etc. Any volunteer?}
\mmt{[MMT: I added some information related to the compactness of the NS and its effects on \hasrem\ in the labeling section]} As the true \ac{EOS} of matter at \ac{NS} densities is unknown, in order to minimize the systematics that arise in adopting a specific \ac{EOS} we consider a set of 23 different \ac{EOS} and marginalize Eqs.~(\ref{bayes-hasns}) and (\ref{bayes-hasrem}) over them. The marginalized probabilities $P_M(\hasns|{\bf A}_{\bf X})$ and $P_M(\hasrem|{\bf A}_{\bf X})$ are defined as
%
\begin{equation}
P_M(I|{\bf A}_{\bf X})=\frac{\sum_J\beta_J P_J(I|{\bf A}_{\bf X})}{\sum_J\beta_J}\,,\label{bayes-marginalized}
\end{equation}
%
where $I=$\hasns\ or \hasrem, $P_J(I|{\bf A}_{\bf X})$ ($J=1,\dots 23$) are the Bayesian probabilities in Eqs.~(\ref{bayes-hasns}) and (\ref{bayes-hasrem}) calculated from the data set $D_S$ with labels assigned according to the $J$-th \ac{EOS}, and $\beta_J$ are Bayes factors \citep{Ghosh:2021eqv}.  \todo{need to say where the Bayes factors come from.} \mmt{[MMT: Reference added]}The probabilities $P_J(I|{\bf A}_{\bf X})$ in Eq.~(\ref{bayes-marginalized}) can be tabulated and used to compute the marginalized probabilities $P_M(\hasns|{\bf A}_{\bf E})$ and $P_M(\hasrem|{\bf A}_{\bf E})$ for any new event $\bf E$ with algorithm outcome $\bf A_{E}$.
