\subsection*{Definition of Event Probabilities}


In the current \ac{LVK} implementation~\cite{Chatterjee:2019avs}, the validity of the classification outcome for an event is assessed through elements of the algorithm's confusion matrix
and its \ac{ROC} curve. For example, given a positive prediction, the ``probability'' that this prediction is correct is usually defined as the \ac{TPR} for the given data set. However,
there is no quantity in the algorithm's confusion matrix that can be used to define a probability in the strictest sense. A second limitation of current \hasns\ and \hasrem\
classification schemas is the fact that the \hasns\ and \hasrem\ labels are treated as independent variables. In reality, the probability of a system being \ac{EM} bright is always
smaller than the probability of the system hosting a \ac{NS}. Therefore, the \hasns\ and \hasrem\ labels cannot be treated as disjoint. One of the main purposes of this work is to go
beyond the above scheme and calculate true \emph{conditional Bayesian probabilities} for \hasns\ and \hasrem. 

Let us define the probability of a system being \ac{EM} bright given the data $X$ as $P(\hasrem|X)$, and the probability of having a \ac{NS} in the system as $P(\hasns|X)$. By
construction, the condition $P(\hasrem|X)\subset P(\hasns|X)$ is never violated when the $P(\hasrem|X)$ and $P(\hasns|X)$ are computed from the ground truth. However, if they are
calculated disjointly from the detection pipeline output, the condition may be violated due to statistical and systematic errors in the pipeline's reconstructed signal parameters, as
well as the \ac{ML} algorithm's bias and limited accuracy. 

There are two possible approaches to avoid the unphysical condition $P(\hasns|X)\subset P(\hasrem|X)$ in the classification output. The first approach consists in introducing a
multilabel scheme that includes the conditional relation \emph{a priori}, and then perform a single, multilabel classification to calculate the probabilities. The second approach
consists in separately classifying the events based on the \hasns\ and \hasrem\ labels, calculate disjoint probabilities for these properties, and then include the conditional relation
\emph{a posteriori}. The first method is the preferred one for efficient multi-label classifiers, such as \ac{KNN} and \ac{RF}. Binary classifiers, or algorithms that are not too efficient in multi-labeling, such as our implementation of \ac{GP}, require the second method to incorporate conditional relations between labels. Let us briefly look at the two schemes in more detail.

In the multilabel classification approach, we label the data into three mutually exclusive categories, where label 0 denotes the class \hasns:\false, label 1 denotes the class
\hasns:\true\ and \hasrem:\false, and label 2 denotes the class \hasns:\true\ and \hasrem:\true. The labels are summarized in Table \ref{table:multilabels}. This scheme eliminates the
possibility of an unphysical classification without further \ac{ML} output processing as there is no category corresponding to \hasrem:\true\ and 
\hasns:\false. The probabilities of having an \ac{EM} bright event and a \ac{NS} in the system are
%
\begin{align}
P(\hasrem|X)&=P(2|X)\,,\label{prem}\\
P(\hasns|X)&=P(2\cap 1|X)\nonumber\\
&=P(2|X)+P(1|X)\nonumber\\
&=1-P(0|X)\,,\label{pns}
\end{align}
%
respectively.

\todo{Table must be made prettier}

\begin{table}[h]\label{tab:multilabels}
\centering
\begin{tabular}{@{}ccc@{}}
\toprule
Class:~\hasns & Class:~\hasrem & Label \\ \midrule
0     & 0      & 0         \\
1     & 0      & 1         \\
1     & 1      & 2         \\ \bottomrule
\end{tabular}
\caption{\tocheck{Labeling adopted for classification of having a NS and having a remnant with the same classifier}}

\end{table}

The above probabilities can be estimated from the data set as follows. According to Bayes theorem, the probability of classifying an event with label $I$, given the data and the
algorithm's \emph{outcome} $A$, is
%
\begin{equation}
P(I|A \cap X)=\frac{P(A \cap X|I)P(I)}{P(A \cap X)}\,,
\label{bayes}
\end{equation}
%
where $P(A \cap X|I)$ is the likelihood of observing the classifier outcome $A$ in the data given the label $I$, $P(I)$ is the probability of observing the label $I$, and $P(A \cap X)$
is the probability of observing the outcome $A$ in the data $X$. The outcome $A$ can be chosen as the value of any classification algorithm variable calculated on the data. For
example, it can be the fraction $f_+$ of \ac{KNN} neighbors or \ac{RF} trees that predict an event to be labeled \hasns:\true\ or \hasrem:\true. Estimators for $P(A \cap X|I)$, $P(I)$, and $P(A \cap X)$ can be obtained from the data set once the algorithm has been trained. Given a data subset independent
from the training set, $\{X_t:X_t\subset X,X_t \cap X_T=\emptyset\}$, where $X_T$ is the training set, the factors in the right hand side of Eq.~(\ref{bayes}) can be estimated as 
%
\begin{align}
P(A \cap X_t|I)&=\frac{N_{{\rm TP}_I}(f_{I_+})}{N_I}\,,\label{bayes-est1}\\
P(I)&=\frac{N_I}{N_{X_t}}\,,\label{bayes-est2}\\
P(A \cap X_t)&=\frac{N_{{\rm TP}_I}(f_{I_+})+N_{{\rm FP}_I}(f_{I_+})}{N_{X_t}}\,,\label{bayes-est3}
\end{align}
%
where $N_{X_{t}}=\hbox{dim}\,(X_t)$, $N_{I}$ is the number of events in $X_t$ with label $I$, and $N_{{\rm TP}_I}(f_{I_+})$ and $N_{{\rm FP}_I}(f_{I_+})$ are the number of events in $X_t$ that are correctly (incorrectly) classified with label $I$ by a fraction $f_{I_+}$ of the chosen outcome variable. Combining Eq.~(\ref{prem}-\ref{bayes-est3}), the probabilities than an event is \ac{EM}-bright or has a \ac{NS} can be written as functions of their corresponding $f_{I_+}$ variables:
%
\begin{align}
P(\hasrem|f_{2_+})=\frac{N_{{\rm TP}_2}(f_{2_+})}{N_{{\rm TP}_2}(f_{2_+})+N_{{\rm FP}_2}(f_{2_+})}\,,\label{PR}\\
P(\hasns|f_{N_+})=1-\frac{N_{{\rm TP}_0}(f_{0_+})}{N_{{\rm TP}_0}(f_{0_+})+N_{{\rm FP}_0}(f_{0_+})}\,.\label{PN}
\end{align}
%
Equations (\ref{PR},\ref{PN}) are generally noisy because they are evaluated on the finite data set $X_t$. Fitting them with sigmoid functions produces smooth probability functions for $P(\hasrem|f_{2_+})$ and $P(\hasns|f_{0_+})$. Given an event, its probabilities of being \ac{EM}-bright or having a \ac{NS} are computed by running the trained classification algorithm on the event and then evaluating the probability functions at $f_{2_+}$ and $f_{0_+}$, respectively.

In the second approach, the probabilities for \hasns\ and \hasrem\ are first calculated as disjoint variables and then the conditional relation $P(\hasrem|X)\subset P(\hasns|X)$ is
folded in. Since we are dealing with two distinct binary classifiers, let us define the probability of an event being \ac{EM} bright (label: $I=$\hasrem) or having a \ac{NS} in the system (label: $I=$\hasns) given the data $X$ and the combined output of the two classifiers, $\{A_1,A_2\}$, as $P(I|\{A_1\cap A_2\}\cap X)$. According to Bayes theorem, $P(I|\{A_1\cap A_2\}\cap X)$ can be written as
%
\begin{equation}
P(I|\{A_1\cap A_2\} \cap X)=\frac{P(\{A_1\cap A_2\}\cap X|I)P(I)}{P(\{A_1\cap A_2\}\cap X)}\,,\\
\label{eq:scheme2}
\end{equation}
%
where $P(\{A_1\cap A_2\}\cap X|I)$ is the likelihood of observing the combined $\{A_1,A_2\}$ outcome of the two classifiers in the data given $I$, $P(I)$ is the probability of observing the $I$, and $P(\{A_1\cap A_2\}\cap X)$ is the probability of observing the combined outcomes in the data $X$. Similar to the multilabel scheme, the algorithm outcomes can be chosen to be any variables of the two classification algorithms. For example, it can be the fractions $f_{1_+}$ and $f_{2_+}$ of the \ac{GP} multivariate expressions predicting an event to be labeled \hasns:\true\ or \hasrem:\true\ from the two classification algorithms. Estimators for the factors in the right hand side of Eq.~(\ref{eq:scheme2}) can be obtained from the data set once the algorithm has been trained as described above in Eq.~(\ref{bayes-est1}-\ref{PN}) for the multilabel scheme with suitable modifications. In this scheme, the probability functions are two-dimensional functions in the $\{f_{1_+},f_{2_+}\}$ space: $P(\hasrem|\{f_{1_+},f_{2_+}\})$ and $P(\hasns|\{f_{1_+},f_{2_+}\})$. They can be smoothed out by a fitting procedure, for example through Gaussian Regression. Given an event, its probabilities of being \ac{EM}-bright or having a \ac{NS} are computed by running the two trained classification algorithms on the event and then evaluating the two probability functions at the same $\{f_{1_+},f_{2_+}\}$ point.

Which approach to choose is ultimately determined by the \ac{ML} algorithm performance. The latter scheme can be implemented when using \ac{ML} algorithms that perform similarly, or
better, in multilabel classification compared to binary classifications. Conversely, if the accuracy of the \ac{ML} for multilabel classification is significantly lower than for binary
classification, it is preferable to adopt the former scheme. This is the case, for instance, for \ac{GP}. Since the accuracy of the \ac{GP} algorithm in its present implementation is
significantly better for binary classifications, in the following we will perform disjoint binary classifications on \hasns\ and \hasrem\ and then use Eq.~\ref{eq:scheme1} to compute
$P(N|X)$ and $P(R|X)$. The classification with \ac{KNN} and \ac{RF} will follow the scheme in Table \ref{tab:multilabels}. 
