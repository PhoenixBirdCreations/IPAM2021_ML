\subsection*{Definition of Event Probabilities}

Let us define the probability of a system having a \ac{NS} and being \ac{EM} bright, given an event in the detector, $X$, and the \emph{outcome} of a classifier, $A$, as $P(\hasns|A\cap X)$ and
$P(\hasrem|A\cap X)$, respectively. Since a system can be EM-bright only when a \ac{NS} is present in the system, the condition $P(\hasrem|A \cap X)\subset P(\hasns|A \cap X)$ must hold. However,
whenever the probabilities are calculated disjointly, as in the current \ac{LVK} implementation, this condition may be violated due to statistical and systematic errors in the pipeline's
reconstructed signal parameters, as well as the \ac{ML} algorithm's bias and limited accuracy. Our approach avoids the occurrence of this inconsistency.

%There are two possible approaches to avoid the unphysical condition $P(\hasns|X)\subset P(\hasrem|X)$ in the classification output. The first approach consists in introducing a
%multilabel scheme that includes the conditional relation \emph{a priori}, and then perform a single, multilabel classification to calculate the probabilities. The second approach
%consists in separately classifying the events based on the \hasns\ and \hasrem\ labels, calculate disjoint probabilities for these properties, and then include the conditional relation
%\emph{a posteriori}. The first method is the preferred one for efficient multi-label classifiers, such as \ac{KNN} and \ac{RF}. Binary classifiers, or algorithms that are not too efficient in multi-labeling, such as our implementation of \ac{GP}, require the second method to incorporate conditional relations between labels. Let us briefly look at the two schemes in more detail.

According to Bayes theorem, the probabilities $P(\hasns|A \cap X)$ and $P(\hasrem|A \cap X)$ can be written as
%
\begin{equation}
P(I|A \cap X)=\frac{P(A \cap X|I)P(I)}{P(A \cap X)}\,,
\label{bayes}
\end{equation}
%
where $I$=\{\hasns, \hasrem\}, $P(A \cap X|I)$ is the likelihood of observing the classifier outcome given $I$, $P(I)$ is the probability of observing $I$, and $P(A \cap X)$
is the probability of observing $A$ for the event $X$. Eq.~(\ref{bayes}) can be estimated from a synthetic data set of labeled events, $D$, as follows. The probability $P(\hasns|A \cap X)$ can be approximated as:  
%
\begin{align}
P(\hasns|A \cap X)&\cong P(\hasns_+|A \cap X_s)\nonumber\\
&=\frac{P(A \cap X_s|\hasns_+)P(\hasns_+)}{P(A \cap X_s)}\,,
\label{bayes-hasns}
\end{align}
%
where \hasns$_+$ is the positive \hasns\ ground truth label for the observed $X_s$ event in $D$ with outcome $A$. The probability $P(\hasrem|A \cap X)$ can be approximated as:
%
\begin{align}
P(&\hasrem|A \cap X)\nonumber\\
&=P(\hasrem|\hasns \cap (A \cap X))P(\hasns|A \cap X)\nonumber\\
%&=P(\hasrem|A \cap (X \cap \hasns))P(\hasns|A \cap X)\nonumber\\
&\cong P(\hasrem_+|A \cap (X_s \cap \hasns_+))P(\hasns_+|A \cap X_s)
\label{bayes-hasrem}
\end{align}
%
where \hasrem$_+$ is the positive \hasrem\ ground truth label for the observed $X_s$ event in $D$.

To evaluate Eqs.~(\ref{bayes-hasns}) and (\ref{bayes-hasrem}) with the \ac{ML} classifier, we divide the synthetic data $D$ in two subsets, $D=D_{TR}\cup D_{TS}$. We use the $D_{TR}$ subset for
algorithm training and validation and the $D_{TS}$ subset to calculate Eq.~(\ref{bayes-est}). Following customary practice, we use a 70\%--30\% split for $D_{TR}$ and $D_{TS}$, respectively.
Depending on the \ac{ML} algorithm, we implement different labeling schemes. The preferred scheme for algorithms that can handle multilabel classification, such as \ac{KNN} and \ac{RF}, produces a
single outcome $A$. This can be obtained by labeling $D$ into three mutually exclusive categories, for example label 0 denoting the class \hasns$_-$, label 1 denoting the class \hasns$_+$ and
\hasrem$_-$, and label 2 denoting the class \hasns$_+$ and \hasrem$_+$. (See Table \ref{tab:multilabels}.) This scheme can be used to evaluate Eqs.~(\ref{bayes-hasns}) and (\ref{bayes-hasrem})
with a single training. The preferred scheme for binary classification algorithms, such as \ac{GP}, is to separately assign labels to \hasns\ and \hasrem, and then separately train two classifiers
on the data set to produce a two-dimensional outcome space.  

~\\\todo{Do we need the table?}\\

\begin{table}[h]
\centering
\begin{tabular}{@{}ccc@{}}
\toprule
Class:~\hasns & Class:~\hasrem & Label \\ \midrule
\false     & \false      & 0         \\
\true     & \false      & 1         \\
\true     & \true      & 2         \\ \bottomrule
\end{tabular}
\caption{\tocheck{Labeling adopted for classification of having a NS and having a remnant with the same classifier}}
\label{tab:multilabels}
\end{table}


The outcome $A$ can be chosen as any value of the classification algorithm variable calculated on the data. For example, $A$ can be the fraction $f_+$ of \ac{KNN} neighbors or \ac{RF}
trees that predict an event to be labeled \hasns$_+$ or \hasrem$_+$. Estimators for the factors in the right-hand side of Eqs.~(\ref{bayes-hasns}) and (\ref{bayes-hasrem}) can be obtained from
$D_{TS}$ once the algorithm has been trained on $D_{TR}$. For example, the factors in Eq.~(\ref{bayes-hasns}) can be estimated as  
%
\begin{align}
&P(\hasns_+)=\frac{N_{\hasns_+}}{N_s}\,,\nonumber\\
&P(A \cap X_s|\hasns_+)=\frac{N^+{}_{\hasns_+}(f_+)}{N_{\hasns_+}}\,,\nonumber\\
&P(A \cap X_s)=\frac{N^+{}_{\hasns_+}(f_+)+N^-{}_{\hasns_+}(f_+)}{N_s}\,,\label{bayes-est}
\end{align}
%
where $N_s=\hbox{dim}\,(D_{TS})$, $N_{\hasns_+}$ is the number of events in the data set with label $\hasns_+$, and $N^+{}_{\hasns_+}(f_+)$ and $N^-{}_{\hasns_+}(f_+)$ are the number of events in $D_{TS}$ that are correctly (incorrectly) classified as ${\hasns_+}$ by the outcome $f_+$. The first factor in the right-hand side of Eq.~(\ref{bayes-hasrem}) can be evaluated similarly to Eq.~(\ref{bayes-est}) by replacing \hasns\ with \hasrem\ and restricting the data set to $D_{TS}(\hasns_+)$.

The probability estimators are generally noisy because they are evaluated on the finite data set $D_{TS}$. To produce smooth probability functions we map them from the $(0,1)$ space to the real line with a logistic function, smooth them with a Savitzky-Golay filter, fit them with a Gaussian Process Regression, and map them back to the $(0,1)$ space. This produces probability tables for 
$P(\hasns|A \cap X)$ and $P(\hasrem|A \cap X)$ than can be used to compute the probabilities for a new event onece the latter has been classified by the algorithm with a given outcome $A$.  

~\\\todo{Marginalization here? Not checked after this point}\\

In the second approach, the probabilities for \hasns\ and \hasrem\ are first calculated as disjoint variables and then the conditional relation $P(\hasrem|X)\subset P(\hasns|X)$ is
folded in. Since we are dealing with two distinct binary classifiers, let us define the probability of an event being \ac{EM} bright (label: $I=$\hasrem) or having a \ac{NS} in the system (label: $I=$\hasns) given the data $X$ and the combined output of the two classifiers, $\{A_1,A_2\}$, as $P(I|\{A_1\cap A_2\}\cap X)$. According to Bayes theorem, $P(I|\{A_1\cap A_2\}\cap X)$ can be written as
%
\begin{equation}
P(I|\{A_1\cap A_2\} \cap X)=\frac{P(\{A_1\cap A_2\}\cap X|I)P(I)}{P(\{A_1\cap A_2\}\cap X)}\,,\\
\label{eq:scheme2}
\end{equation}
%
where $P(\{A_1\cap A_2\}\cap X|I)$ is the likelihood of observing the combined $\{A_1,A_2\}$ outcome of the two classifiers in the data given $I$, $P(I)$ is the probability of observing the $I$, and $P(\{A_1\cap A_2\}\cap X)$ is the probability of observing the combined outcomes in the data $X$. Similar to the multilabel scheme, the algorithm outcomes can be chosen to be any variables of the two classification algorithms. For example, it can be the fractions $f_{1_+}$ and $f_{2_+}$ of the \ac{GP} multivariate expressions predicting an event to be labeled \hasns:\true\ or \hasrem:\true\ from the two classification algorithms. Estimators for the factors in the right hand side of Eq.~(\ref{eq:scheme2}) can be obtained from the data set once the algorithm has been trained as described above in Eq.~(\ref{bayes-est1}-\ref{PN}) for the multilabel scheme with suitable modifications. In this scheme, the probability functions are two-dimensional functions in the $\{f_{1_+},f_{2_+}\}$ space: $P(\hasrem|\{f_{1_+},f_{2_+}\})$ and $P(\hasns|\{f_{1_+},f_{2_+}\})$. They can be smoothed out by a fitting procedure, for example through Gaussian Regression. Given an event, its probabilities of being \ac{EM}-bright or having a \ac{NS} are computed by running the two trained classification algorithms on the event and then evaluating the two probability functions at the same $\{f_{1_+},f_{2_+}\}$ point.

Which approach to choose is ultimately determined by the \ac{ML} algorithm performance. The \mmt{former} scheme can be implemented when using \ac{ML} algorithms that perform similarly, or
better, in multilabel classification compared to binary classifications. Conversely, if the accuracy of the \ac{ML} for multilabel classification is significantly lower than for binary
classification, it is preferable to adopt the \mmt{latter} scheme. This is the case, for instance, for \ac{GP}. Since the accuracy of the \ac{GP} algorithm in its present implementation is
significantly better for binary classifications, in the following we will perform disjoint binary classifications on \hasns\ and \hasrem\ and then use Eq.~\ref{eq:scheme1} to compute
$P(N|X)$ and $P(R|X)$. The classification with \ac{KNN} and \ac{RF} will follow the scheme in Table \ref{tab:multilabels}. 
