\subsection*{Definition of Event Probabilities}

Let us define the probability of a system having a \ac{NS} (\hasns=\true) and being \ac{EM} bright (\hasrem=\true), given an event $X$ and the outcome of a classifier on its detection pipeline
output, ${\bf A}_{\bf X}$, as $P(\hasns|{\bf A}_{\bf X})$ and $P(\hasrem|{\bf A}_{\bf X})$, respectively. The classifier outcome can be understood as a map $A:{\bf X}\to {\bf A}_{\bf X}$, where
$\bf X$ is a vector that identifies the output of the detection pipeline and ${\bf A}_{\bf X}$ is a vector that uniquely identifies the algorithm's output on $\bf X$. 

Since a system can be EM-bright only when a \ac{NS} is present in the system, the condition $P(\hasrem|{\bf A}_{\bf X})\subset P(\hasns|{\bf A}_{\bf X})$ must hold. However, if the probabilities
are calculated disjointly, as in the current \ac{LVK} implementation, this condition may be violated due to statistical and systematic errors in the pipeline's reconstructed signal parameters, as
well as the \ac{ML} algorithm's bias and limited accuracy. The approach discussed below avoids the occurrence of this inconsistency.

The ground truth of observed events is unknown. Therefore, the probabilities $P(\hasns|{\bf A}_{\bf X})$ and $P(\hasrem|{\bf A}_{\bf X})$ cannot be calcuated from observations. However,
estimators for $P(\hasns|{\bf A}_{\bf X})$ and $P(\hasrem|{\bf A}_{\bf X})$ can be calculated from from synthetic events under the assumption that these events are a faithful representation of
real observations as follows.

Let us consider a data set of $N$ simulated events ${\bf X}'_i$, $D=\{{\bf X}'_i\otimes{\bf L}({\bf X}'_i)\}$, where $i=1\dots N$ and ${\bf L}$ is a map from ${\bf X}'$ to a vector space
that defines labels for the \hasns\ and \hasrem\ ground truths. According to Bayes theorem, $P(\hasns|{\bf A}_{\bf X})$  can be written as 
%
\begin{equation}
P(\hasns|{\bf A}_{\bf X})=\frac{P({\bf A}_{\bf X}|\hasns)P(\hasns)}{P({\bf A}_{\bf X})}\,,
\label{bayes}
\end{equation}
%
where $P({\bf A}_{\bf X}|\hasns)$ is the likelihood of observing the classifier's outcome given an event with a \ac{NS} in the system, $P(\hasns)$ is the probability that a system includes a \ac{NS}, and $P({\bf A}_{\bf X})$ is the probability of observing the classifier outcome ${\bf A}_{\bf X}$. The probability $P(\hasns|A_{\bf X})$ can be approximated as:  
%
\begin{align}
P(\hasns|{\bf A}_{\bf X})&\simeq P(\hasns_+|{\bf A}_{{\bf X}'})\nonumber\\
&=\frac{P({\bf A}_{{\bf X}'}|\hasns_+)P(\hasns_+)}{P({\bf A}_{{\bf X}'})}\,,
\label{bayes-hasns}
\end{align}
%
where $\hasns_+={\bf L}[{\bf X}'(\hasns=\true)]$ is a label that uniquely identifies elements in $D$ with positive \hasns\ ground truth, ${\bf X}'$ is an element in $D$, and ${\bf A}_{{\bf X}'}$ is the outcome of the classifier on ${\bf X}'$. The probability $P(\hasrem|{\bf A}_{\bf X})$ can be approximated as:
%
\begin{align}
P(&\hasrem|{\bf A}_{\bf X})\nonumber\\
&=P(\hasrem|\hasns, {\bf A}_{\bf X})P(\hasns|{\bf A}_{\bf X})\nonumber\\
%&\simeq P(\hasrem_+|\hasns_+,A_{{\bf X}'})P(\hasns_+|A_{X'})\nonumber\\
&\simeq P(\hasrem_+|{\bf A}_{{\bf X}''})P(\hasns_+|{\bf A}_{{\bf X}'})
\label{bayes-hasrem}
\end{align}
%
where $\hasrem_+={\bf L}[{\bf X}'(\hasrem=\true)]$ is a label that uniquely identifies elements in $D$ with positive \hasrem\ ground truth and ${\bf X}''= {\bf X}'(\hasns=\true)$.

To evaluate Eqs.~(\ref{bayes-hasns}) and (\ref{bayes-hasrem}) with an \ac{ML} classifier, the synthetic data set is divided into two subsets, $D=D_R\oplus D_S$. The $D_R$ subset is used for
algorithm training and validation. The $D_S$ subset is used to estimate the probabilities. Following customary practice, throughout this paper we use a 70\%--30\% split for $D_R$ and $D_S$,
respectively.

The choice of the labeling scheme and the algorithm's outcome depend on the \ac{ML} algorithm characteristics. The preferred scheme for multilabel algorithms, such as \ac{KNN} and \ac{RF}, is a
labeling map with each element in $D$ classified into mutually exclusive categories. The classifier outcome can then be chosen as a two-dimensional vector space  ${\bf A}_{{\bf X}'}\subset
\mathbb{R}^3$. This scheme allows the calculation of Eqs.~(\ref{bayes-hasns}) and (\ref{bayes-hasrem}) with a single training process. For example, one can label data as  ${\bf L}[{\bf
X}'(\hasns=\false)]=0$, ${\bf L}[{\bf X}'(\hasns=\true,\hasrem=\false)]=1$, and ${\bf L}[{\bf X}'(\hasns=\true,\hasrem=\true)]=2$ (see Table \ref{tab:multilabels}). With this labeling,
\hasrem$_+=2$ and \hasns$_+=1\cup 2$, therefore a suitable choice for the algorithm outcome is ${\bf A}_{\bf X}=[f_1({\bf X}')+f_2({\bf X}'),f_2({\bf X}')]\subset[f_0({\bf X}'),f_1({\bf X}'),f_2({\bf X}')]$, where $f_0=1-f_1-f_2$, $f_1$, and $f_2$ are the fractions of \ac{KNN} neighbors or \ac{RF} trees that predict the event to have labels 0, 1, and 2, respectively.

The preferred scheme for binary classification algorithms that cannot efficiently handle multilabel classifications, such as \ac{GP}, is to separately label the events as  ${\bf L}({\bf
X}'_i)={\bf L}_\hasns({\bf X}'_i)\otimes {\bf L}_\hasrem({\bf X}'_i)$ and separately train for \hasns\ and \hasrem. In this scheme $\hasns_+={\bf L}_\hasns[{\bf X}'(\hasns=\true)]$ and $\hasrem_+={\bf L}_\hasrem[{\bf X}'(\hasrem=\true)]$. As for the previous scheme, the physical condition $P(\hasrem|{\bf A}_{\bf X})\subset P(\hasns|{\bf A}_{\bf X})$ is ensured by the conditional probability definition for $P(\hasrem|{\bf A}_{\bf X})$ in Eq.~(\ref{bayes-hasrem}).

~\\\todo{Do we need the table?}\\

\begin{table}[h]
\centering
\begin{tabular}{@{}ccc@{}}
\toprule
Class:~\hasns & Class:~\hasrem & Label \\ \midrule
\false     & \false      & 0         \\
\true     & \false      & 1         \\
\true     & \true      & 2         \\ \bottomrule
\end{tabular}
\caption{\tocheck{Labeling adopted for classification of having a NS and having a remnant with the same classifier}}
\label{tab:multilabels}
\end{table}

The factors in the right-hand side of Eqs.~(\ref{bayes-hasns}) and (\ref{bayes-hasrem}) can be obtained from $D_{S}$ once the algorithm has been trained on $D_{R}$. For example, for the \ac{KNN} and \ac{RF} scheme described earlier, the factors in Eq.~(\ref{bayes-hasns}) can be estimated as  
%
\begin{align}
&P(\hasns_+)=\frac{N_{\hasns_+}}{N_s}\,,\nonumber\\
&P(A_Xs|\hasns_+)=\frac{N^+{}_{\hasns_+}(f_1+f_2)}{N_{\hasns_+}}\,,\nonumber\\
&P(A_Xs)=\frac{N^+{}_{\hasns_+}(f_1+f_2)+N^-{}_{\hasns_+}(f_1+f_2)}{N_s}\,,\label{bayes-est}
\end{align}
%
where $N_s=\hbox{dim}\,(D_S)$, $N_{\hasns_+}$ is the number of events in the data set with label $\hasns_+$, and $N^+{}_{\hasns_+}(f_1+f_2)$ and $N^-{}_{\hasns_+}(f_1+f_2)$ are the number of events in $D_S$ that are correctly (incorrectly) classified as ${\hasns_+}$ by the outcome $f_1+f_2$. The first factor in the right-hand side of Eq.~(\ref{bayes-hasrem}) can be evaluated similarly to Eq.~(\ref{bayes-est}) by replacing \hasns\ with \hasrem\ and restricting the data set to elements with \hasns$_+$ label.

The probability estimators are generally noisy because they are evaluated on the finite data set $D_S$. To produce smooth probability functions we map them from the $(0,1)$ space to the real line with a logistic function, smooth them with a Savitzky-Golay filter, fit them with a Gaussian Process Regression, and map them back to the $(0,1)$ space. This produces probability tables for 
$P(\hasns|{\bf A}_{\bf X})$ and $P(\hasrem|{\bf A}_{\bf X})$ than can be used to compute the probabilities for a new event onece the latter has been classified by the algorithm with a given outcome $A$.  

~\\\todo{Describe marginalization here}\\
