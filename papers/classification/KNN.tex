\subsection{K-Nearest Neighbors (KNN)}

\mmt{One of the non-parametric algorithms we have used for classification is the \texttt{KNeighborsClassifier} (KNN). This algorithm assumes that similar things are near to each other.  It captures the idea of similarity by computing the distance between points in a graph.  It is also a lazy learning algorithm, because it doesnâ€™t have a training phase, it uses all the data for training while classification -the dataset is stored and at the time of classification, it acts on the dataset. } 

\mmt{The workflow of the KNN algorithm is the following: first,  after splitting the data into training and testing sets, we need to select the number of neighbors, $K$,  which can be any integer.  Then, for each point of the testing dataset we compute the distance between the point and each row of the training data with a chosen metric (Euclidean, Manhattan, Chebysev, Mahalanobis), and we sort the points in ascending order based on the distance value.  By choosing the top $K$ neighbors from the sorted array, we assign a class to the test point, which is the most frequent class of the chosen neighbors. }
