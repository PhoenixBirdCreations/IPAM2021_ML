\subsection*{K-Nearest Neighbors}

K-Nearest Neighbors is a non-parametric, supervised algorithm~\cite{Fix:1951,Cover:1967} that uses the fact that similar points in a data set are ``near'' each other in their
parameter space. When it is applied to classification problems~\cite{Guo:2004}, the algorithm is usually renamed \ac{KNN}. The algorithm captures the idea of similarity between
points by computing the distance between each point in a training set and its neighbors according to a pre-determined metric. Next, it sorts the neighbors in ascending order based on
their distance to the testing point. By choosing the top $K$ neighbors from the sorted array, \ac{KNN} assigns the label to the testing point that corresponds to the most
frequent neighbor.

In this work, we use the open-source Python \ac{KNN} implementation of scikit-learn~\cite{Pedregosa:2011ork}. We fix the algorithm hyperparameters by
cross-validating over the data set and obtain the highest accuracy. Throughout our analysis we use $K = 8$ neighbors, the Manhattan metric, the BallTree
algorithm and the neighbors are weighted by the inverse of their distance to the event. \tocheck{This configuration differs from the \ac{LVK}'s current
implementation, which employs the Mahalanobis metric and $K = 2n + 1 = 11$ neighbors, where $n$ is the number of features. Our configuration is the optimal
choice for the new labeling scheme that is presented in Sect.~\ref{labeling}).}

%\mmt{Marina: I would remove this part and just say the options that we fix. I don't think we need to describe the options in the library.
%In this implementation, the distance between the different neighbors can be
%computed using different metrics: Euclidean, Manhattan, Chebyshev, and Mahalanobis~\cite{ADictionaryofStatistics}. The prediction can be weighted by the inverse of the distance or equally
%weighted in the neighborhood of each data point. The nearest neighbors can be estimated with different methods: BallTree, KDTree, or a brute-force search~\cite{Bentley1975,Omohundro1989}. All these parameters are tunable to optimize the algorithm for the problem at hand. Following Ref.~\cite{Chatterjee:2019avs}, we fix the optimal number of neighbors to $K \approx 2n+1$,
%where $n$ is the number of features of each data point. According to usual practice, we cross-validate the prediction by computing the accuracy, defined as the fraction of correctly classified events, over a range of possible values for the parameters of the algorithm.}




