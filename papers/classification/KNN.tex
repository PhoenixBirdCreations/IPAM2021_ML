\section{K-Nearest Neighbors (KNN)}

One of the non-parametric algorithms we have used for classification is the \texttt{KNeighborsClassifier} (KNN). This algorithm assumes that similar things are near to each other.  It captures the idea of similarity by computing the distance between points in a graph.  It is also a lazy learning algorithm, because it does not have a training phase, it uses all the data for training while classification -the dataset is stored and at the time of classification, it acts on the dataset. 

The workflow of the KNN algorithm is the following: first,  after splitting the data into training and testing sets, we need to select the number of neighbors, $K$,  which can be any integer.  Then, for each point of the testing dataset we compute the distance between the point and each row of the training data with a chosen metric (\texttt{euclidean, manhattan, chebysev, mahalanobis}), and we sort the points in ascending order based on the distance value.  By choosing the top $K$ neighbors from the sorted array, we assign a class to the test point, which is the most frequent class of the chosen neighbors.
