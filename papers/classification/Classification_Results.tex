\section{Results} \label{results}

In this section, we first discuss the performance of the trained \ac{RF} and \ac{KNN} classifiers on the testing data set $D_S$ and use the latter to derive the Bayesian probabilities $P_M(I|{\bf
A})$. Then we evaluate $P_M(I|{\bf A})$ on two independent data sets. The first data set includes a population of simulated \ac{CBC} events that were injected in the real-time replay of \ac{O3} data. This set was used for the \ac{LVK} \ac{MDC}~\cite{Chaudhary:2023vec}. The second set contains the confident \ac{LVK} \ac{O3} detections that are reported in \ac{LVK}'s \ac{GWTC3}
catalog~\cite{LIGOScientific:2021djp}.

\subsection{Performance of the algorithms}

We measure the performance of our algorithms by the \ac{TPR} and the \ac{FPR} on the events in $D_S$, drawn as \ac{ROC} curves, obtained from the scores provided by the algorithms. The \ac{ROC} curves show the variation of the true-positive rate with the false-positive rate given thresholds for the scores.  

\begin{figure*}[h]
\includegraphics[width=0.45\linewidth]{roc_testing_KNN_NS}
\includegraphics[width=0.45\linewidth]{roc_testing_KNN_REM}
\caption{\ac{ROC} curves obtained from the \ac{O2} testing data set $D_S$ for the \ac{KNN} classifier (left: \hasns, right: \hasrem). The curves for the 23 different \ac{EOS}s are displayed in
gray, with the curves for {\tt BHF\_BBB2}, {\tt MS1\_PP}, and {\tt SLy} highlighted in red, green, and blue, respectively. The circle, triangle, and square markers denote score thresholds of
$0.1$, $0.5$, and $0.9$, respectively.}
\label{fig:rocO2_KNN}
\end{figure*}

\begin{figure*}[h]
\includegraphics[width=0.45\linewidth]{roc_testing_RF_NS}
\includegraphics[width=0.45\linewidth]{roc_testing_RF_REM}
\caption{\ac{ROC} curves obtained from the \ac{O2} testing data set $D_S$ for the \ac{RF} classifier (left: \hasns, right: \hasrem). The curves for the 23 different \ac{EOS}s are displayed in
gray, with the curves for {\tt BHF\_BBB2}, {\tt MS1\_PP}, and {\tt SLy} highlighted in red, green, and blue, respectively. The circle, triangle, and square markers denote score thresholds of
$0.1$, $0.5$, and $0.9$, respectively.}
\label{fig:rocO2_RF}
\end{figure*}

The left and right panels of Fig.~\ref{fig:rocO2_KNN} show the \hasns\ and \hasrem\ \ac{ROC} curves for the \ac{KNN} algorithm, respectively. The analogous curves for the \ac{RF}
classifier are shown in Fig.~\ref{fig:rocO2_RF}. The \ac{ROC} curves for the 23 \ac{EOS} are plotted in grey with three of them highlighted in color: {\tt BHF\_BBB2}, the \ac{EOS}
with lowest maximum mass for the NS, {\tt MS1\_PP}, the \ac{EOS} with largest maximum mass for the \ac{NS}, and {\tt SLy}, which allows for a maximum mass of $2.05 M_\odot$ and is
the standard \ac{EOS} used in \ac{LVK} low-latency investigations~\cite{Ghosh:2021eqv}. The markers denote different thresholds for the Bayesian probabilities $P_M(I|{\bf A})$. 


The two classifiers perform consistently across all \ac{EOS}s. The \ac{TPR} for a score threshold of $0.5$ is around $0.99$ for both \hasns\ and \hasrem. A comparison of the
\hasns\ and \hasrem\ \ac{ROC} curves for each algorithm shows that the \ac{FPR} for \hasns\ is generally higher than the \ac{FPR} for \hasrem\ at a given threshold. Thus the algorithms
typically do a better job in classifying \hasrem\ than \hasns. A separate comparison of the \ac{KNN} and \ac{RF} \ac{ROC} curves for \hasns\ and \hasrem\ shows that both algorithms perform
similarly on the O2 testing data set, with \ac{RF} giving slightly higher \ac{TPR} and lower \ac{FPR} than \ac{KNN} at a fixed threshold. 

\subsection{Computation of the Bayesian probabilities}

Once the algorithms are trained and tested, we compute the Bayesian
probabilities defined in the Section~\ref{bayesian_probs}, in terms of the 
output of the algorithms (fractions of neighbors and fractions of trees, 
respectively). In order to obtain the probabilities given in 
Equations~\eqref{bayes-hasns} and~\eqref{bayes-hasrem}, one needs to apply
 the algorithm on known events, which are the events from the O2 data set $D$ already used for training and testing the algorithms in the previous subsection. More concretely, we evaluate these probabilities on the testing set $D_S$.

\begin{figure*}%[h]
\includegraphics[width=0.45\linewidth]{KNN_3_eos_prob_plots_HasNS}
\includegraphics[width=0.45\linewidth]{KNN_3_eos_prob_plots_HasRem}
\caption{Left panel: Bayesian probability fit for \hasns\ as a function of the fraction of \ac{KNN} neighbors $f_1+f_2$. Right panel: Bayesian probability fit for \hasrem\ as a function of the fraction of \ac{KNN} neighbors $f_2$. Fits for the {\tt BHF\_BBB2}, {\tt MS1\_PP}, and {\tt SLy} \ac{EOS}s are highlighted in red, green, and blue, respectively. The fits exhibit an upward trend with the fraction of neighbors.  Non-monotonic fluctuations are due to the data set's finite size.}
\label{fig:bayesian_prob_fits_KNN}
\end{figure*}

\begin{figure*}%[h]
\includegraphics[width=0.45\linewidth]{RF_3_eos_prob_plots_HasNS}
\includegraphics[width=0.45\linewidth]{RF_3_eos_prob_plots_HasRem}
\caption{Left panel: Bayesian probability fit for \hasns\ as a function of the fraction of \ac{RF} trees $f_1+f_2$. Right panel: Bayesian probability fit for \hasrem\ as a function of the fraction of \ac{RF} Trees $f_2$. Fits for the {\tt BHF\_BBB2}, {\tt MS1\_PP}, and {\tt SLy} \ac{EOS}s are highlighted in red, green, and blue, respectively. The fits exhibit an upward trend with the fraction of trees.  Non-monotonic fluctuations are due to the data set's finite size.}
\label{fig:bayesian_prob_fits_RF}
\end{figure*}

In Figs.~\ref{fig:bayesian_prob_fits_KNN} and~\ref{fig:bayesian_prob_fits_RF}, 
we show the fits of the results with \ac{GPR},
performed as explained in the Section~\ref{bayesian_probs}. In these figures, we show
the fits for each EOS, but one can always get a single marginalized probability
applying the Bayes' factors. The fitted curves generally increase with the number of neighbors/trees, except for some fluctuations due to the fact that the data set is finite resulting in some noise. The fits also have a
sigmoid-like shape, but that depends on the choice of the EOS. Both \ac{KNN}
and \ac{RF} give fits that have a similar shape. Indeed, both show a slower
increase of $P_J(\hasrem |\bf{A_X})$ for a low fraction $f_2$ compared to $P_J(\hasns |\bf{A_X})$.
However, \ac{RF} shows a small plateau on the $P_J(\hasns |\bf{A_X})$ (and also on
$P_J(\hasrem |\bf{A_X})$) for fractions of trees around $0.8$, which 
\ac{KNN} does not reproduce. We highlight the same equations of state as in
Figs.~\ref{fig:rocO2_KNN} and~\ref{fig:rocO2_RF}.

\subsection{Performance on new events}

As stated in Section~\ref{bayesian_probs}, the fits of the probabilities $P_J(I|{\bf A}_{\bf X})$ for each EOS ($J=1,\dots 23$) can be tabulated and used to compute the marginalized
probabilities $P_M(\hasns|{\bf A}_{\bf E})$ and $P_M(\hasrem|{\bf A}_{\bf E})$ for any new event $\bf E$ with algorithm outcome $\bf A_{E}$.  We classify the events from the MDC data set and compute the ROC
curves according to the ground truth of the injected events, which can be seen in Figs.~\ref{fig:rocMDC_KNN} and~\ref{fig:rocMDC_RF}. In contrast to the \ac{O2} data set, the \ac{MDC} set contains
outputs from different matched-filtering pipelines. Therefore, we separate the ROC curves for the different pipelines involved in the data set (GstLAL,  PyCBC~\cite{Usman:2015kfa}, SPIIR~\cite{Chu:2020pjv} and
MBTA~\cite{Adams:2015ulm}). 

%Notice that the curves are very similar even though the algorithms were trained (and also the Bayesian probabilities were fitted) using exclusively GstLAL.

Figure~\ref{fig:rocMDC_KNN} depicts the curves for \ac{KNN}.  In the case of
\hasns\ (left panel), it gives a true positive rate of $\sim 0.975$ for a
threshold of $0.5$ and a false positive rate of less than $0.2$ for all the
pipelines except of SPIIR, for which the performance of the algorithms is poorer. This behaviour with SPIIR data is also reported in other studies~\cite{Chaudhary:2023vec} and might need further investigation. Regarding \hasrem\ (right panel), \ac{KNN} gives a
false positive rate slightly higher than $0.2$ for all the pipelines except of
GstLAL when fixing the threshold to $0.5$, but the true positive rate stays
around $0.975$. In this case, SPIIR behaves similarly to the rest of the pipelines. 

In Fig.~\ref{fig:rocMDC_RF} we showcase the results for \ac{RF}.  The curves
are similar to the ones in Fig.~\ref{fig:rocMDC_KNN}, having a steeper shape for \hasns\ (left panel) and thus giving high true positive rates and low false positive rates at a given threshold. \ac{RF} also performs poorly for SPIIR. When it comes to \hasrem\ (right panel), the false positive rates increase as for \ac{KNN}, and it outperforms on GstLAL compared to the rest of the pipelines.  

One can note that, when applied to a new data set, the algorithms perform better on \hasns\ rather than on \hasrem\, as opposed to what happened with the testing data set $D_S$ from O2 (see Figs.~\ref{fig:rocO2_KNN} and~\ref{fig:rocO2_RF}). Moreover, \ac{KNN} outperforms \ac{RF} when it comes to \hasrem\ (especially on pipelines other than GstLAL), and both algorithms work better for events injected on GstLAL, which is the pipeline exclusively used in the O2 training data set, $D_R$. Due to the hard cuts applied by \ac{RF} and the fact that it has been trained only with GstLAL injections, this algorithm is less adaptable than \ac{KNN}, which is why the ROC curves are not as steep as for \ac{KNN} when classifying data from other pipelines. 

%\begin{comment}
%performs very well for \hasns achieving a true positive rate very close to
%unity with a very small false positive rate. We observe that SPIIR pipeline
%deviates from the good behaviour. For \hasrem the overall performance is
%slightly worse, getting higher false positive rate, but in turn all
%pipelines behave equally good. In Fig.~\ref{fig:rocMDC_RF} we showcase the results for \ac{RF}. As with \ac{KNN} the results are very good, with steep ROC curves. For \hasns spiir pipeline deviates less from the rest, getting higher positive rates for the same false positives than with \ac{KNN}. In the case of \hasrem the curves of the different pipelines behave a bit more differently from each other.
%\end{comment}

\begin{figure*}%[h]
\includegraphics[width=0.45\linewidth]{roc_mdc_KNN_NS}
\includegraphics[width=0.45\linewidth]{roc_mdc_KNN_REM}
\caption{\ac{ROC} curves obtained from the \ac{O3} \ac{MDC} data set for the \ac{KNN} classifier (left: \hasns, right: \hasrem). The different \ac{LVK} matched-filtering pipelines are indicated by different colors (GstLAL: red; PyCBC: green; gold: SPIIR; blue: MBTA). The results for all pipelines are shown in black. The circle, triangle, and square markers denote probability thresholds of $0.1$, $0.5$, and $0.9$, respectively.}
\label{fig:rocMDC_KNN}
\end{figure*}


\begin{figure*}%[h]
\includegraphics[width=0.45\linewidth]{roc_mdc_RF_NS}
\includegraphics[width=0.45\linewidth]{roc_mdc_RF_REM}
\caption{\ac{ROC} curves obtained from the \ac{O3} \ac{MDC} data set for the \ac{RF} classifier (left: \hasns, right: \hasrem). The different \ac{LVK} matched-filtering pipelines are indicated by different colors (GstLAL: red; PyCBC: green; gold: SPIIR; blue: MBTA). The results for all pipelines are shown in black. The circle, triangle, and square markers denote probability thresholds of $0.1$, $0.5$, and $0.9$, respectively.}
\label{fig:rocMDC_RF}
\end{figure*}

We also applied the tabulated Bayesian probabilities to classify real events
from the O3 LVK run~\cite{LIGOScientific:2020ibl, LIGOScientific:2021djp}.  In Table~\ref{tab:real_data_bayesian} we depict some of the most significant events, labeled with their event ID.  We show the marginalized Bayesian probabilities for both \hasns\ and \hasrem\ computed with 
\ac{RF} and \ac{KNN}. For the first two events (GW170817 and GW190425),
$P_M(\hasns |\bf{A_E})$ and $P_M(\hasrem |\bf{A_E})$ are very close to 1, meaning that these events
correspond to BNS mergers, as shown in~\cite{LIGOScientific:2020ibl,LIGOScientific:2021djp}. 
For GW190426 and GW200115, $P_M(\hasns |{\bf{A_E}}) \sim 1$, but
$P_M(\hasrem |{\bf{A_E}}) = 0$ (they are actually non-zero, but smaller than $10^{-3}$), which correspond to NSBH mergers, and agree 
with~\cite{LIGOScientific:2020ibl, LIGOScientific:2021djp}. Finally, GW190814 and GW190924,
have non-zero $P_M(\hasns |\bf{A_E})$ (indeed, \ac{KNN} gives a probability higher than
0.5 for GW190814). The results in~\cite{LIGOScientific:2020ibl,
LIGOScientific:2021djp} show that these events are high mass-ratio BBH mergers.
The fact that the component masses are very different from each other (and the
lower mass is close to the NS limit) might have led to these non-zero
probabilities. Moreover, for GW190814 \ac{KNN} gives a probability higher than
0.5 for \hasns, which differs considerably from what \ac{RF} predicts, which is
only a 0.042. This is because for this event, regarding at the posterior of the
secondary mass, only three out of twenty three equations of state are
compatible with a NS. Since \ac{RF} applies a hard cut on the EOS,
$P_M(\hasns|{\bf A}_{\bf E})$ will be low. However, \ac{KNN} depends on the
neighbors surrounding the event, and the mass gap parameter space is not well
covered in the training data set, which makes the closer neighbors NSs. 

\begin{table}[]
\begin{tabular}{c|cc|cc}
\hline
\multicolumn{1}{c|}{}      & \multicolumn{2}{c|}{$P_M(\hasns|{\bf A}_{\bf E})$}                                                & \multicolumn{2}{c}{$P_M(\hasrem|{\bf A}_{\bf E})$}                                                \\ \hline
\multicolumn{1}{c|}{event ID}   & \multicolumn{1}{c}{RF} & \multicolumn{1}{c}{KNN}  & \multicolumn{1}{c}{RF} & \multicolumn{1}{c}{KNN} \\ \hline
GW170817                                   & 0.998                   & 0.989                    & 0.997                   & 0.985                                  \\
GW190425                                   & 0.998                   & 0.989                    & 0.997                   & 0.985                            \\
GW190426                                   & 0.997                   & 0.985                    & 0.000                   & 0.000                     \\
GW190814                                   & 0.042                   & 0.567                   & 0.000                  & 0.000                      \\
GW190924                                   & 0.012                   & 0.054                   & 0.000               & 0.000                       \\               
GW200115                                   & 0.998                   & 0.989                   & 0.000                  & 0.000                           \\
\hline
\end{tabular}
\caption{Bayesian probabilities of some real GW events from the O3 observing run. GW170817 and GW190425 actually correspond to BNS mergers; GW190426 and GW200115 are NSBH mergers, and GW190814 and GW190924 are BBH mergers. The reason why $P_M(\hasns|{\bf A}_{\bf E})$ is not zero for GW190814 and GW190924 is because the inferred secondary mass is located close to the threshold value of the mass of a NS. The probabilities are truncated to $10^{-3}$, which means that there is not a zero Bayesian probability, but it is actually smaller than $10^{-3}$.}
\label{tab:real_data_bayesian}
\end{table}

In Figs.~\ref{fig:param_sweep_KNN} and~\ref{fig:param_sweep_RF} we depict parameter sweeps that are built using the Bayesian probabilities computed with \ac{KNN} and \ac{RF}, respectively.  These figures show the Bayesian probabilities for \hasns\ (left column) and for \hasrem\ (right column) for given component masses of the binary system, $M_1$ being the larger mass. The rows stand for different fixed values of the component spins. The SNR is fixed to 10. One can see that both algorithms perform in a similar way. Regarding $P_M(\hasns |\bf{A_E})$,  there are no notable differences between the choice of spin values.  There is a small difference between the algorithms: for the third row ($\chi_1 = 0.0$ and $\chi_2=1.0$), $P_M(\hasns |\bf{A_E})$ is larger for bigger $M_1$ in the case of \ac{KNN}. For $P_M(\hasrem |\bf{A_E})$, both algorithms give almost the same results. The sweeps show that, for a large $\chi_1$, the probability of having a remnant increases for bigger values of the first component mass, $M_1$. As expected, the region in which $P_M(\hasrem |\bf{A_E}) \sim 1$ is inside the region in which $P_M(\hasns |\bf{A_E}) \sim 1$.

Moreover, the parameter sweeps of $P_M(\hasns |\bf{A_E})$ (left column) for \ac{KNN} seem to be noisier for large primary masses. This is because the algorithm just looks at the closest neighbors, and it only needs few neighbors that are labelled differently to provide a higher probability in a certain region. On the other hand, \ac{RF} applies a hard cut on large primary masses, resulting in a more uniform probability in that region. 

\begin{figure*}%[h]
\includegraphics[width=0.7\linewidth]{KNN_parameter_sweep}
    \caption{Parameter sweep obtained with \ac{KNN}. The Bayesian probabilities are shown as a function of the component masses of the binary, where $M_1$ (the bigger mass) is in the horizontal axes and $M_2$, in the vertical. The left panels show $P_M(\hasns |\bf{A_E})$ and the right panels, $P_M(\hasrem |\bf{A_E})$. We choose different values of the spin components to evaluate the probability, and they appear in the different rows of the figure.   The SNR is fixed to 10. }
\label{fig:param_sweep_KNN}
\end{figure*}

\begin{figure*}%[h]
\includegraphics[width=0.7\linewidth]{RF_parameter_sweep}
 \caption{Same as Figure~\ref{fig:param_sweep_KNN}, but now for \ac{RF}.}
\label{fig:param_sweep_RF}
\end{figure*}


%Here are the results for the methods:

%\input{KNN_Results.tex} %plots and comments
%\input{RF_Results.tex} %plots and comments
%\input{GP_Results.tex}


%In order to decide which method gives a better performance in classifying this kind of events, we can apply them over testing data and finally do a comparison between both. A way to see how data is classified we can construct histograms where the number of events that are classified with a label (\texttt{HasNS/HasRemnant}) \texttt{True} or \texttt{False} will change with a given threshold of the probability. For an algorithm with perfect performance, all the events with label \texttt{True (False)} should be at \textit{p}(\texttt{label}) = 1 (\textit{p}(\texttt{label}) = 0).

%Another way to check the algorithm's performance is by building the so-called \textit{Receiver Operating Characteristic (ROC) Curve}. They show the variation of the true-positive rate (or efficiency) with the false-positive rate given a certain threshold for the probability. An algorithm with a proper performance will give a steeper ROC curve, or in other words, will have a higher eficiency with a lower false-positive rate.  


%In the ROC curves that we will present in the following subsections, we highlight three reference EoS in color, from which we show results in more detail. We select BHF\_BBB2 because is the model that give the lowest maximum mass, MS1\_PP as the model with the bigger maximum mass, and we also include SLy because is the most accepted EoS for NS modeling (reference), and is the one that was used in the injections that are our dataset.


%\subsection{Algorithm comparison}


%Here we talk about overall results and specifically from each algorithm in the
%subsections below.  \mmt{[MMT: Below I described how to check the performance of the algorithms. Maybe a table with all the scores/sensitivities/precisions from both %KNN and RF would be useful (already got it in a google doc)]}

%\mmt{To measure the performance of the classifiers we use some common statistical quantities.  The score is the number of correctly predicted events over the number %of total events (a perfect classifier has a score of 1).  It works best when there is an equal number of events for each label in the training set. It does not %consider the importance of misclassification, or that the training data can be biased towards one specific label.}

%\mmt{The mean score is computed by training the algorithm on the $90\%$ of the dataset and testing it on the remaining $10\%$, cycling the train/test combination over %the full dataset. To do that, we are going to use the training dataset, since it's the larger one.  In order to train and test the model and create the different %plots, we are going to use the training and testing files. }

%\mmt{Another useful quantity is the sensitivity. It is the ratio between the true positives and the sum of the true positives and false negatives.  It measures how %much the algorithm predicts \textit{true} (in our case it would be that the event has NS or has REM), when it is actually \textit{true}.Having a sensitivity equal to %1 would mean that our method predicts \textit{true} for every event. Therefore, a method with high sensitivity will barely miss true alarms. }

%\mmt{A quantity that measures how much you can trust a method when it predicts \textit{true} is the precision.  It is the ratio between the true positives and the %some of the true  and the false positives. A precision equal to 1 means that the method never predicts \textit{true} when it is actually \textit{false}. This means %that the method will never give false alarms. }

%\mmt{Finally, the F1 score $F1 = 2(\rm{precision \times sensitivity})/(\rm{precision+sensitivity})$ is a type of score that takes into consideration how precision and %sensitivity compensate each other. A perfect classifier would have a F1 score of 1.}

%To compare quantitatively the results from RF and KNN we compute the true positive and false positive rate for several threshold values, for both HasNS and HasREM, for the three selected EoS. These are tables \ref{tab:TPbhf}, \ref{tab:TPms1} and \ref{tab:TPsly}. For HasNS the two algorithms perform similarly, with almost the same TP for all threshold values and accross EoSs, although the false positive is smaller always in the RF. For HasREM we obtain that RF performs better than KNN in every case, with not only a smaller false positive rate, but a greater true positive rate.

%\begin{table}[]
%\centering
%\begin{tabular}{@{}c|cccc|cccc@{}}
%\toprule
%\multicolumn{1}{l|}{}          & \multicolumn{4}{c|}{Has NS}                       & \multicolumn{4}{c}{Has REM}                      \\ \midrule
%                               & \multicolumn{2}{c}{RF} & \multicolumn{2}{c|}{KNN} & \multicolumn{2}{c}{RF} & \multicolumn{2}{c}{KNN} \\
%\multicolumn{1}{l|}{Threshold} & TP         & FP        & TP          & FP         & TP         & FP        & TP         & FP         \\ \midrule
%0.1                            & 0.999      & 0.107     &   0.999          &  0.156          & 0.998      & 0.011     &    0.992        &  0.051          \\
%0.3                            & 0.998      & 0.068     &   0.996        &  0.117          & 0.993      & 0.005     &   0.974         &  0.017          \\
%0.5                            & 0.994      & 0.042     &   0.991          &  0.088           & 0.985      & 0.003     &   0.937         &  0.006          \\
%0.8                            & 0.967      & 0.014     &   0.966          & 0.043            & 0.957      & 0.001     &  0.845          &   0.001         \\ %\bottomrule
%\end{tabular}
%\caption{BHF\_BB2}
%\label{tab:TPbhf}
%\end{table}

