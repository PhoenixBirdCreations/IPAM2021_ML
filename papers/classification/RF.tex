\subsection*{Random Forests}

\ac{RF} is a classification method based on an ensemble of decision trees. The trees are hierarchical models that make decisions by recursively splitting the data into different
categories based on the values of features. Each tree in the forest is trained independently, and the algorithm combines the results to assign a class \tocheck{not sure I understand this:
``based on the modes from the trees.'' What are the modes? There is no mention of modes so far.} The probability of an event belonging to a class is given by the fraction of trees that
correctly predict it. \ac{RF} is known for its parallelization capabilities, as computations inside each tree are independent from the rest. An \ac{RF} algorithm is usually trained using
bootstrapping, a technique that randomly assigns subsets of the training dataset to each tree. This helps prevent overfitting, as each individual classifier is not exposed to the same
data, and encourages pattern recognition by studying the same data from different subsets. The model's performance on the given data set can be optimized by tuning the input
hyperparameters.

In our analysis, we use the \ac{RF} implementation of scikitlearn \cite{Pedregosa:2011}. The main hyperparameters to tune in this module are the number of trees, the maximum allowed depth, and the information gain criteria used at splitting. \todo{I'm not sure I understand this. Is this something we fix, or something fixed by the scikit implementation?} We fix the maximum number of features considered in a node  as the square root of the total number of features. Low-memory usage is one of the requirements for the implementation of \ac{ML} algorithms in low-latency \ac{LVK} products. Therefore, we restrict the maximum number of trees to 300 and their maximum depth to 45 before tuning the hyperparameters. The optimal hyperparameters are chosen by measuring the performance of the algorithm's score, which is defined as the fraction of correctly classified events \tocheck{still this is not clear. no threshold has been mentioned so far: using a threshold of 0.5 to assign a category.}


 
%\todo{Do we need all the details here? They seem too much for what we need. Just give the main features, maybe in the previous paragraph.} The training of the \ac{RF} algorithm is usually done with bootstrap, a technique that assigns a random subset of the training dataset to each tree. This prevents overfitting as every individual classifier is not exposed to the same data, and encourages pattern recognition by studying the same data from different subsets. Every decision tree is composed by nodes, where data its splitted until the different categories are separated. At each node, a subset of the features of the data is selected along threshold values that maximize the information gain at the separation. The binary splitting at each node gives the tree its name, as it can be visualized as roots going deeper at separations.



