\subsection*{Random Forests}

\ac{RF} is a classification method based on an ensemble of decision trees. \todo{Need a sentence here saying how it works} \marina{, which are  hierarchical models that makes decisions by recursively splitting the data into different categories based on the values of features.} \marina{\sout{One of the major strengths of the algorithm is that}} \todo{The following sentences are badly written. Please clean them up} \marina\sout{every train trains and classifies independently from the rest, while the RF classification joints all the results and assign as category the mode from the trees. The probability of belonging to a category is therefore straightforward, being the number of trees that chose it divided by the total number of trees. Notice that the training and evaluation of a RF can be accelerated by parallelization, as computations inside each tree are independent from the rest.}}
\marina{Each tree in the forest is trained, and classifies, independently, and the algorithm combines the results to assign a category based on the mode from the trees. The probability of belonging to a category can be easily calculated by dividing the number of trees that chose it by the total number of trees. RF is known for its parallelization capabilities, as computations inside each tree are independent from the rest. The particularities of the training are determined by the input hyperparameters, which allow to search for the optimum \ac{RF} for a dataset. One of this hyperparameters is to allow bootstrap, a technique tyat randomly assigns subsets of the training dataset to each tree. This helps prevent overfitting, as each individual classifier is not exposed to the same data, and encourages pattern recognition by studying the same data from different subsets. }

\todo{Do we need all the details here? They seem too much for what we need. Just give the main features, maybe in the previous paragraph.} \marina{\sout{The training of the \ac{RF} algorithm is usually done with bootstrap, a technique that assigns a random subset of the training dataset to each tree. This prevents overfitting as every individual classifier is not exposed to the same data, and encourages pattern recognition by studying the same data from different subsets. Every decision tree is composed by nodes, where data its splitted until the different categories are separated. At each node, a subset of the features of the data is selected along threshold values that maximize the information gain at the separation. The binary splitting at each node gives the tree its name, as it can be visualized as roots going deeper at separations.}}

In our analysis, we use the \ac{RF} implementation of scikitlearn \citep{Pedregosa:2011}. The main hyperparameters \marina\sout{\todo{``hyperparameters'' need to be mentioned earlier in the description}}} to tune in this module are the number of trees, the maximum allowed depth, and the information gain criteria used at splitting. \todo{I'm not sure I understand this. Is this something we fix, or something fixed by the scikit implementation? The maximum number of features in a node is fixed as the square root of the total number of features.} Low-memory usage is one of the requirements for the implementation of \ac{ML} algorithms in low-latency \ac{LVK} products. Therefore, we restrict the maximum number of trees to 300 and their maximum depth to 45 before tuning the hyperparameters. The optimal hyperparameters are chosen by measuring the performance of the algorithm's score, which is defined as the fraction of correctly classified events  \todo{which threshold? if threshold is taken as 0.5.} \todo{Not sure I understand this sentence: As all categories are balanced, this approach is enough to roughly compare models.}

\todo{This will be moved to later: For the RF we use events with 5 features: the two masses, their corresponding spins and the SNR of the detection.}

 



