\subsection*{Random Forests}

\ac{RF} is a classification method based on an ensemble of decision trees. The trees are hierarchical models that make decisions by recursively splitting the data at the separation nodes into different categories based on the values of features. Each tree in the forest is trained independently. To classify a data point, each tree predicts a category and the algorithm assigns the one that has been chosen the most. \ac{RF} is known for its parallelization capabilities, as computations inside each tree are independent from the rest. An \ac{RF} algorithm is usually trained using bootstrapping, a technique that randomly assigns subsets of the training dataset to each tree. This helps prevent overfitting, as each individual classifier is not exposed to the same data, and encourages pattern recognition by studying the same data from different subsets. The model's performance on the given data set can be optimized by tuning the input hyperparameters.

In this work, we use the open-source Python \ac{RF} implementation of scikit-learn \cite{Pedregosa:2011}. The main tunable hyperparameters are the number of trees, the maximum allowed depth and the information gain criteria used at splitting. We fix the maximum number of features considered in a node as the square root of the total number of features. Low-memory usage is one of the requirements for the implementation of \ac{ML} algorithms in low-latency \ac{LVK} products. Therefore, we restrict the maximum number of trees to 300 and their maximum depth to 45 before tuning the hyperparameters. Similar to \ac{KNN}, we choose the algorithm's optimal hyperparameters by measuring the accuracy in the testing set.
