\subsection*{Random Forest}

\ac{RF} is a classification method based on an ensemble of decision trees. The trees are hierarchical models that make decisions by recursively splitting the data at the separation nodes
into different categories based on the values of features. Each tree in the forest is trained independently. To classify a data point, each tree predicts a category and the algorithm
assigns the one that has been chosen the most. \ac{RF} is known for its parallelization capabilities, as computations inside each tree are independent from the rest. An \ac{RF} algorithm
is usually trained using bootstrapping, a technique that randomly assigns subsets of the training data set to each tree. This helps prevent overfitting, as each individual classifier is
not exposed to the same data, and encourages pattern recognition by studying the same data from different subsets. The model's performance on the given data set can be optimized by tuning
the input hyperparameters.

In this work, we use the open-source Python \ac{RF} implementation of scikit-learn~\cite{Pedregosa:2011ork}. The main tunable hyperparameters are the number of trees, the maximum allowed
depth, and the information gain criteria used at splitting. Similar to \ac{KNN}, we choose the algorithm's optimal hyperparameters by measuring the accuracy in the testing set. We use 50
trees in the forest, with a maximum depth of 15, and the maximum number of features considered in a node is the square root of the total number of features.

