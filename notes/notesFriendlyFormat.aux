\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Constraints on the output}{2}{section.2}\protected@file@percent }
\newlabel{sec:constraints}{{2}{2}{Constraints on the output}{section.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  \textbf  {Left panel}: activation functions to use in the output layer so that the output is constrained in the interval $[-1,1]$. \textbf  {Right panel}: history of a NN with two hidden layers with 20 neurons and $\sigma ^5(x)$ as output function. The drop in the accuracy at the 18th epoch is due to the presence of NaN. That's why we use $\mathaccentV {hat}05E{\sigma }$ and not a smooth approximant $\sigma ^N$.}}{2}{figure.1}\protected@file@percent }
\newlabel{fig:out_act_f}{{1}{2}{\textbf {Left panel}: activation functions to use in the output layer so that the output is constrained in the interval $[-1,1]$. \textbf {Right panel}: history of a NN with two hidden layers with 20 neurons and $\sigma ^5(x)$ as output function. The drop in the accuracy at the 18th epoch is due to the presence of NaN. That's why we use $\hat {\sigma }$ and not a smooth approximant $\sigma ^N$}{figure.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Some results}{2}{section.3}\protected@file@percent }
\newlabel{sec:some_results}{{3}{2}{Some results}{section.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  Results of the regression on the \texttt  {v0c0} dataset. We used 2 hidden layers with 100 neurons, $N_{\rm  batch}=64$, 250 epochs, constrained output. See Sec.\nobreakspace  {}\ref  {sec:some_results} for discussion. The colors in the first plots is related to the absolute difference between predicted and injected. The black line is the bisector.}}{3}{figure.2}\protected@file@percent }
\newlabel{fig:v0c0_results}{{2}{3}{Results of the regression on the \texttt {v0c0} dataset. We used 2 hidden layers with 100 neurons, $N_{\rm batch}=64$, 250 epochs, constrained output. See Sec.~\ref {sec:some_results} for discussion. The colors in the first plots is related to the absolute difference between predicted and injected. The black line is the bisector}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  Results of the regression on the \texttt  {v1c0} dataset. We used 2 hidden layers with 100 neurons, $N_{\rm  batch}=64$, 250 epochs, constrained output. See Sec.\nobreakspace  {}\ref  {sec:some_results} for discussion. The colors in the first plots is related to the absolute difference between predicted and injected. The black line is the bisector.}}{4}{figure.3}\protected@file@percent }
\newlabel{fig:v1c0_results}{{3}{4}{Results of the regression on the \texttt {v1c0} dataset. We used 2 hidden layers with 100 neurons, $N_{\rm batch}=64$, 250 epochs, constrained output. See Sec.~\ref {sec:some_results} for discussion. The colors in the first plots is related to the absolute difference between predicted and injected. The black line is the bisector}{figure.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces  Loss (Mean Squared Error) and $R^2$ coefficients for the two datasets \texttt  {v0c0}, \texttt  {v1c0}. We used 2 hidden layers with 100 neurons, $N_{\rm  batch}=64$, 250 epochs, constrained output. See Sec.\nobreakspace  {}\ref  {sec:some_results} for discussion.}}{5}{table.1}\protected@file@percent }
\newlabel{tab:some_results}{{1}{5}{Loss (Mean Squared Error) and $R^2$ coefficients for the two datasets \texttt {v0c0}, \texttt {v1c0}. We used 2 hidden layers with 100 neurons, $N_{\rm batch}=64$, 250 epochs, constrained output. See Sec.~\ref {sec:some_results} for discussion}{table.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Cross validation on layers}{5}{section.4}\protected@file@percent }
\newlabel{sec:crossval}{{4}{5}{Cross validation on layers}{section.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  $R^2$ coefficients for different architectures. We show only the models that have an $R^2$ coefficient above the threshold indicated in the title of each panel. We show the results for the mean of $R^2$ and for the $R^2$ of each feature. In the rainbow-scatter plot the green circle marks the highest $R^2$ coefficient. }}{6}{figure.4}\protected@file@percent }
\newlabel{fig:crossval_v0c0}{{4}{6}{$R^2$ coefficients for different architectures. We show only the models that have an $R^2$ coefficient above the threshold indicated in the title of each panel. We show the results for the mean of $R^2$ and for the $R^2$ of each feature. In the rainbow-scatter plot the green circle marks the highest $R^2$ coefficient}{figure.4}{}}
