\documentclass[prd,aps,twocolumn,a4paper,showkeys,nofootinbib]{revtex4-1}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}	
\usepackage{graphicx}
\usepackage{color}
\allowdisplaybreaks

\def\TODO{\textcolor{red}{TODO:}}

\begin{document}

\title{Regression with Dense Deural Network using TensorFlow}

\author{Simone \surname{Albanesi}}

\date{\today}

\maketitle

%==========================================================
\section{Introduction}
%==========================================================
Working notes on the regression part with a dense Neural Network (NN)
using \texttt{Keras} with \texttt{TensorFlow} in back-end. We use this framework since allows us to
exploit the flexibility of NNs (while \texttt{Scikit-learn} doesn't).

The task is to apply the NN to the quantities recovered from \texttt{GstLAL} $x$ and to predict
values $y^{\rm pred} = f\left( x \right)$ that are a better approximation of the injected 
values $y^{\rm true}$.

Some pros of NNs are:
\begin{itemize}
\item virtually infinite number of architectures, many aspects to tune on our problem 
\item they are used everywhere nowadays, so it is easy to find material online
\item is possible to put constraints on the output
\item easy to customize
\end{itemize}
On the other hand, to fully exploit the NN we need a big dataset and the high number of tunable
aspects makes the cross validation more difficult. In our case we will use at most 2 hidden layers.
In principle more complex architectures could  be used but in our case 
I don't think it would be worth since we do not have a huge traning sample. 
For regression tasks, the activation function in the output layer generally is a linear function.

Steps of the regression:
\begin{enumerate}
\item \textbf{Normalization of the data}. The usual normalization (\texttt{StandardScaler})
trasforms the data so that we have zero mean and standard deviation equal to 1. 
Nonetheless I prefer to use the 
linear map \texttt{MinMaxScaler} that maps the data in the interval $[-1,1]$. 
This choice is linked to the constraints on the output, see Sec.~\ref{sec:constraints}. 

\item \textbf{Inizialization of the model}. We decide the architecture and all the other aspects. 
A typical configuration is:
\begin{itemize}
\item layers: two layers with 100 neurons, but e.g. on the \texttt{v0c0}
dataset a more complex architecture could be slightly better 
(see Sec.~\ref{sec:crossval} for more info on this);
\item activation function in hidden layers: \texttt{ReLU}. Other options like the sigmoid 
or the hyperbolic tangent require longer training and provide worse results.;
\item activation function in output layer: \texttt{linear};
\item optimizer: \texttt{Adam} (optimized version of the stochastic gradient descent);
\item loss function: \texttt{MeanSquaredError()} (see Sec.~\ref{sec:loss} for more details 
and other options);
\item learning rate: 0.001;
\item epochs and batch-size: a good combination is 250 and 32, respectively. Sometimes I use a
bigger batch-size to speed-up the training (e.g. I used 128 for the cross-validation on
the \texttt{v0c0} dataset).
\end{itemize}

\item \textbf{Training}. Using  the dataset \texttt{v0c0}, \texttt{epochs=250} and 
\texttt{batch{\_}size=128} the trainig
takes from 60 to 90 seconds, depending on the architecture. During the training we also 
use the $20\%$ of the training dataset for the validation so that
we can monitor the loss and the $R^2$ coefficient (see definition below) at each epoch.

\item \textbf{Evaluation of the accuracy}. To evaluate the goodness of the model 
we use the coefficient of determination $R^2=1-SS_{\rm res}/SS_{\rm tot}$, where
$SS_{\rm res}=\sum{(y^{\rm true}_i-f(x_i))^2}$, 
$SS_{\rm tot}=\sum{(y^{\rm true}_i - \bar{y}^{\rm true})^2}$, 
and $\bar{y}^{\rm true}$ is the mean. This is also the \texttt{score} 
of \texttt{Scikit-learn}. It is better to evaluate the $R^2$ coefficient
for each feature instead of a global $R^2$. 
\end{enumerate}

%==========================================================
\section{Constraints on the output}
%==========================================================
\label{sec:constraints}
We need some constraints on the output since we don't want to predict negative masses 
or naked singularities. A solution is to modify the activation function 
$\sigma$ in the output layer.
Another option could be to enforce the constraints in the loss function, but I have not tried this
(and in any case this would not guarantee physical predictions). 
The idea is to make $\sigma$ saturate when the prediction is out of the physical range. 
The first step is to normalize the data in the interval $[-1,1]$ using the \texttt{MinMaxScaler}.
Then for $\sigma$ we use the following prescription:
\begin{equation}
\hat{\sigma}(x) = \begin{cases} 
      -1 & x\leq -1 \\
      x & -1\leq x\leq 1 \\
      1 & x \geq 1 
\end{cases}
\end{equation}
Using a sigmoid ansatz is possible to find a smooth approximants
\begin{equation}
\sigma^N(x)  = \frac{2}{e^{-2 f^N(x)}+1}-1,
\end{equation}
where 
\begin{equation}
f^N(x)  = \sum_{n=odd}^{N} \frac{x^{n}}{n}.
\end{equation}
The function $f^N(x)$ is found requiring that $\sigma^N(x)=x+O(x^{N+2})$ for $x\rightarrow 0$.
These activation functions are shown in the left panel of Fig.~\ref{fig:out_act_f}.
%
\begin{figure}[t]
  \center
  \includegraphics[width=0.23\textwidth,height=3.7cm]{./Figs/out_activation_function.png}
  \includegraphics[width=0.23\textwidth,height=3.9cm]{./Figs/history_sigma5_layers20+20.png}
  \caption{\label{fig:out_act_f} \textbf{Left panel}: activation functions to use in the 
  output layer so that the output is constrained in the interval $[-1,1]$.
  \textbf{Right panel}: history of a NN with two hidden layers with 20 neurons 
  and $\sigma^5(x)$ as output function.
  The drop in the accuracy at the 18th epoch is due to the presence of NaN.}
\end{figure}
%
However, for $N\geq 3$ is possible that some NaN appear during the training, see e.g. the right
panel of Fig.~\ref{fig:out_act_f} where we show a case with $\sigma^5(x)$. 
I remember that once also $\sigma^3(x)$ failed, but I 
cannot find this configuration again (is a rare event). 
In any case to be safe I will use the piecewise function $\hat{\sigma}$ ($\sigma^1(x)$ is 
not steep enough).
Clearly it is also possible to saturate only the upper of the lower boundary, in particular we 
for the masses I will often use
\begin{equation}
\hat{\sigma}_{\rm LB}(x) = \begin{cases} 
      -1 & x\leq -1 \\
      x & -1\leq x.
\end{cases}
\end{equation}
However I am not sure that removing the upper boundary makes sense, maybe it would 
be better to use the upper constraint also for the masses, TBD.
%
\begin{table}[t]
\caption{\label{tab:some_results} Loss (Mean Squared Error) and $R^2$ coefficients
 	for the two datasets \texttt{v0c0}, \texttt{v1c0}.
   	We used 2 hidden layers with 100 neurons, \texttt{batch{\_}size=64}, 250 epochs,  
   	constrained output. See Sec.~\ref{sec:some_results} for discussion.}
\begin{center}
\begin{ruledtabular}
\begin{tabular}{c | c | c} 
 & \texttt{v0c0} & \texttt{v1c0}  \\
\hline
\hline
loss (MSE)      & 0.0079 & 0.0533 \\
\hline
$R^2$ mean      & 0.9652 & 0.7533 \\
\hline
$R^2 [m_1   ]$  & 0.9962 & 0.8938 \\
$R^2 [m_2   ]$  & 0.9860 & 0.9454 \\
$R^2 [s^1_x ]$  & 0.9582 & 0.6134 \\
$R^2 [s^1_y ]$  & 0.9529 & 0.6344 \\
$R^2 [s^1_z ]$  & 0.9549 & 0.6436 \\
$R^2 [s^2_x ]$  & 0.9539 & 0.6853 \\
$R^2 [s^2_y ]$  & 0.9570 & 0.6409 \\
$R^2 [s^2_z ]$  & 0.9568 & 0.6468 \\
$R^2 [\theta]$  & 0.9073 & 0.6648 \\
$R^2 [q     ]$  & 0.9957 & 0.9244 \\
$R^2 [M_c   ]$  & 0.9978 & 0.9938 
\end{tabular}
\end{ruledtabular}
\end{center}
\end{table}

%==========================================================
\section{Some results}
%==========================================================
\label{sec:some_results}
%
\begin{figure*}[t]
  \center
  \includegraphics[width=0.95\textwidth]{./Figs/v0c0_regression.png}
  \includegraphics[width=0.95\textwidth]{./Figs/v0c0_regression_noise.png}
  \caption{\label{fig:v0c0_results} Results of the regression on the \texttt{v0c0}
  dataset. We used 2 hidden layers with 100 neurons, \texttt{batch{\_}size=64}, 250 epochs,  
  constrained output. See Sec.~\ref{sec:some_results} for discussion. 
  The colors in the first plots is related to the absolute difference between predicted 
  and injected. The black line is the bisector.}
\end{figure*}
%
\begin{figure*}[t]
  \center
  \includegraphics[width=0.95\textwidth]{./Figs/v1c0_regression.png}
  \includegraphics[width=0.95\textwidth]{./Figs/v1c0_regression_noise.png}
  \caption{\label{fig:v1c0_results} Results of the regression on the \texttt{v1c0}
  dataset. We used 2 hidden layers with 100 neurons, \texttt{batch{\_}size=64}, 250 epochs,  
  constrained output. See Sec.~\ref{sec:some_results} for discussion.
  The colors in the first plots is related to the absolute difference between predicted 
  and injected. The black line is the bisector.}
\end{figure*}
%
The regression with NN works pretty well on the \texttt{v0} datasets, while on 
the dataset \texttt{v1} the $R^2$ coefficients are smaller since there is more degeneracy.
We use 2 hidden layers with 100 neurons (i.e. 12,411 trainable parameters, while
$N_{\rm sample}=2\cdot 10^4$) and we train on 250 epochs using 
\texttt{batch{\_}size=64}. Different architectures are discussed in Sec.~\ref{sec:crossval}
but the results are more or less the same for complex-enough NNs.
For the spin components we use $\hat{\sigma}$ as output 
activation function, for the masses and the mass ratio we use $\hat{\sigma}_{\rm LB}$ and
a linear function for the angle. 
In Fig.~\ref{fig:v0c0_results}, Fig.~\ref{fig:v1c0_results} and Table~\ref{tab:some_results}
there are the results for this model.
In the first plots we plot the predicted value against the injected values, while in the seconds
we plot the prediction (orange) on the recovered (blue). Note that the recovered here are not 
physically consistent but it is ok since the main goal at this point 
is to see if a neural network is able to 
recover the original quantities. The relevant thing is that we are able to predict values
that have physical meaning.

%==========================================================
\section{Cross validation on layers}
%==========================================================
\label{sec:crossval}
In order to decide the architecture to use I did a cross validation on the 
\texttt{v0c0} dataset (the reason why I have not cross-validated on the \texttt{GstLAL} dataset
is explained in Sec.~\ref{sec:GstLAL}). I trained the models on 250 epochs using
\texttt{batch{\_}sizes=128} for different architectures. The results are shown 
in Fig.~\ref{fig:crossval_v0c0}. The plots show that NNs with 2 layers with $\sim 100$ 
neurons each provide good enough scores and more complex NNs do not provide much better results. 
After a certain number of trainable parameters the $R^2$ coefficient reaches a plateaux 
and doesn't increase anymore. 
The only exceptions are: (i) $M_c$: the best score is obtained with single-layer NNs 
(the recovered quantity is obtained only adding random gaussian noise); 
(ii) $\theta$: this is the quantity with lowest score and the $R^2$ coefficient 
continues to (slightly) increase without reaching a plateaux even for $\sim 300$
neurons in each layer. 

Finally note that many NNs have more parameters thant training samples, 
$N_{\rm param}>N_{\rm train}$, since 
$N_{\rm train}=2\cdot 10^4$ for the \texttt{v0v0} dataset. This shouldn't be a problem 
for NN (and indeed the score is good also on the test-set that has 
$N_{\rm test}=1.5\cdot 10^4$), however we could also decide to use smaller NNs 
since the accuracy in more complex NNs is not drastically better than in simpler models. 
For example in Fig.~\ref{fig:v0c0_results} and Fig.~\ref{fig:v1c0_results} I used
two hidden layers with 100 neurons so that $N_{\rm param}=12411<N_{\rm sample}=2\cdot 10^4$.

%
\begin{figure*}[t]
  \center
  \includegraphics[width=0.45\textwidth]{./Figs/crossval_v0c0_R2mean.png}
  \hspace{0.3cm}
  \includegraphics[width=0.45\textwidth]{./Figs/crossval_v0c0_R2m1.png} \\
  \includegraphics[width=0.45\textwidth]{./Figs/crossval_v0c0_R2m2.png}
  \hspace{0.3cm}
  \includegraphics[width=0.45\textwidth]{./Figs/crossval_v0c0_R2s1x.png} \\
  \includegraphics[width=0.45\textwidth]{./Figs/crossval_v0c0_R2s1y.png}
  \hspace{0.3cm}
  \includegraphics[width=0.45\textwidth]{./Figs/crossval_v0c0_R2s1z.png} \\
  \includegraphics[width=0.45\textwidth]{./Figs/crossval_v0c0_R2s2x.png} 
  \hspace{0.3cm}
  \includegraphics[width=0.45\textwidth]{./Figs/crossval_v0c0_R2s2y.png} \\
  \includegraphics[width=0.45\textwidth]{./Figs/crossval_v0c0_R2s2z.png} 
  \hspace{0.3cm}
  \includegraphics[width=0.45\textwidth]{./Figs/crossval_v0c0_R2theta.png} \\
  \includegraphics[width=0.45\textwidth]{./Figs/crossval_v0c0_R2q.png} 
  \hspace{0.3cm}
  \includegraphics[width=0.45\textwidth]{./Figs/crossval_v0c0_R2Mc.png} 
  \caption{\label{fig:crossval_v0c0} $R^2$ coefficients for different architectures. 
  We show only the models that have an $R^2$ coefficient
  above the threshold indicated in the title of each panel. We show the results
  for the mean of $R^2$ and for the $R^2$ of each feature. In the rainbow-scatter plot
  the green circle marks the highest $R^2$ coefficient. }
\end{figure*}
%

%==========================================================
\section{GstLAL BNS dataset}
%==========================================================
\label{sec:GstLAL}
We also have a dataset obtained with injected parameters of nonspinning 
BNSs recovered with \texttt{GstLAL}.
In this case we have only three features: $m_1$, $m_2$, $M_c$.
The regression on this dataset is more problematic:
\begin{itemize}
\item The output of the regression for the two masses $m_i$ seems almost independent on the 
architecture and is not improved with more complex NNs. The result for $M_c$ instead is 
more sensible on the architecture (but the regression seems useless for $M_c$)

\item The predictions of $m_i$ have a sharp edge in correspondence of the region with 
most degeneracy, see Fig.~\ref{fig:GstLAL_results}

\item The model stops to learn after the second epoch. Training for more epochs only improves
the prediction of the lowest and highest chirp masses, but does not improve the prediction
of $m_i$. 

\item The mass ratio computed with the predicted masses is in a very narrow range while
the injection have a much wider interval, see the last panel of Fig.~\ref{fig:GstLAL_results}.

\end{itemize} 

Therefore in this case the regression does not seem to work properly. I have also tried to 
do the regression only on $m_{1,2}$ and $M_c$ and recovering the other mass analytically 
but the results are the same. Even SVR gives the same results (qualitatively).
%
\begin{figure*}[t]
  \center
  \includegraphics[width=0.95\textwidth]{./Figs/GstLAL_regression.png}
  \includegraphics[width=0.95\textwidth]{./Figs/GstLAL_regression_noise.png}
  \includegraphics[width=0.31\textwidth]{./Figs/GstLAL_m2.png}
  \includegraphics[width=0.31\textwidth]{./Figs/GstLAL_Mc.png}
  \includegraphics[width=0.31\textwidth]{./Figs/GstLAL_q.png}
  \caption{\label{fig:GstLAL_results} Results of the regression on the \texttt{GstLAL}
  dataset. We used one hidden layer with 100 neurons, \texttt{batch{\_}size=64}, 50 epochs,  
  unconstrained output.
  The colors in the first plots is related to the absolute difference between predicted 
  and injected. The black line is the bisector.
  The final $R^2$ coefficients for $m_1$, $m_2$ and $M_c$ 
  are 0.7363, 0.7819, and 0.9999, respectively.
  In the last three panels we plot $m_2$, $M_c$ and $q$ over $m_1$ for the injected, recovered
  and predicted quantities.}
\end{figure*}

%==========================================================
\section{Loss function}
%==========================================================
\label{sec:loss}
In order to fix the problem with $q$ in the \texttt{GstLAL} dataset, 
I tried to modify the loss function to minimize.
The default function for regression tasks is the Mean Squared Error
\begin{equation}
J_{\rm mse} = {\rm mean}\left( \sum_i \left( y^{\rm true}_i - f(x_i) \right)^2 \right).
\end{equation}
I tried to modify it including a $q$-penalty and a $M_c$-penalty:
\begin{align}
J =  {\rm mean} & \left(  \sum_i \left( y^{\rm true}_i - f(x_i) \right)^2 \right.   \nonumber \\
&  + \lambda_{\rm q} \sum_i \left( q^{\rm true}_i - q^{\rm pred}_i \right)^2
\nonumber \\
& \left. + \lambda_{\rm M_c} \sum_i \left( M_{c,i}^{\rm true} - M_{c,i}^{\rm pred} \right)^2 \right)
\end{align}
where 
\begin{align}
q^{\rm pred}_i &  = \frac{m_{2,i}^{\rm pred}}{m_{1,i}^{\rm pred}}, \\
M_{c,i}^{\rm pred} & = 
\frac{\left(m_{1,i}^{\rm pred}m_{2,i}^{\rm pred} \right)^{3/5}}{\left(m_{1,i}^{\rm pred}+m_{2,i}^{\rm pred}\right)^{1/3}},
\end{align}
and $q^{\rm true}$, $M_c^{\rm true}$ are analogous. Note the $M_c$-term is not the same 
in MSE since here the chirp mass is computed using $m_i$ while in the MSE part $M_c$ 
is the third feature.
In order to have the predicted masses in the loss function we need to 
rescale $x$ but since the loss function is defined using the Keras backend and 
for the moment this is implemented only for  \texttt{MinMaxScaler}. 
We cannot use \texttt{numpy} in the Keras backend, so the loss function is not fully vectorized
(but this is not a big issue). 
I tried to fit the data with different values for the hyperparameters $\lambda_q$ and
$\lambda_Mc$ but in every case I didn't find a solution to the issues of Sec.~\ref{sec:GstLAL}
since the effect of these penalty is marginal, even if we use high values for $\lambda_i$.
Maybe this is not the correct way to include penalties.

%==========================================================
\section{Things to discuss}
%==========================================================
\begin{itemize}
\item The regression with NNs does not work properly on the GstLAL BNS dataset.
Also Yanyan had some problems on this dataset (BTW I don't know if it is the exact same
proble). Probably linked to the strong degeneracy in the recovery of the masses, 
but we need to (i) fully understand why this happens, (ii) fix it. 
However it's possible that this problem won't be there
with a dataset with masses in a much broader range (or maybe will be there anyway, who knows).

\item NN and SVR have the same problem on the GstLAL BNS dataset and the only 
similarity between the two algorithms is that they both minimize a loss function. 
To my understanding GPR has a completely different approach: how does it work on this dataset? 
Does it have the same issues? 

\item Maybe we could try to simulate the \texttt{GstLAL}'s noise in a more realistic way
for \texttt{v0c0} and \texttt{v1c0}, but I am not sure that this would be useful since 
we already have a GstLAL dataset (even if only with BNS).

\item Since we are starting to use more complex NNs (at least for \texttt{NewRealistic} 
datasets, i.e. \texttt{v0c0} and \texttt{v1c0}), should we try to use some dropout?
[not a first order priority in any case]
\end{itemize}
%==========================================================
\bibliography{refs,local}
%==========================================================

\end{document}


