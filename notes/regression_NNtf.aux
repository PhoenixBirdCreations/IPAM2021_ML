\relax 
\newlabel{FirstPage}{{}{1}{}{}{}}
\@writefile{toc}{\contentsline {title}{Regression with Dense Deural Network using TensorFlow}{1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Constraints on the output}{1}{}}
\newlabel{sec:constraints}{{II}{1}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  \textbf  {Left panel}: activation functions to use in the output layer so that the output is constrained in the interval $[-1,1]$. \textbf  {Right panel}: history of a NN with two hidden layers with 20 neurons and $\sigma ^5(x)$ as output function. The drop in the accuracy at the 18th epoch is due to the presence of NaN. That's why we use $\mathaccentV {hat}05E{\sigma }$ and not a smooth approximant $\sigma ^N$.}}{2}{}}
\newlabel{fig:out_act_f}{{1}{2}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Some results}{2}{}}
\newlabel{sec:some_results}{{III}{2}{}{}{}}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces  Loss (Mean Squared Error) and $R^2$ coefficients for the two datasets \texttt  {v0c0}, \texttt  {v1c0}. We used 2 hidden layers with 100 neurons, $N_{\rm  batch}=64$, 250 epochs, constrained output. See Sec.\nobreakspace  {}III{}{}{}\hbox {} for discussion.}}{2}{}}
\newlabel{tab:some_results}{{I}{2}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Cross validation on layers}{2}{}}
\newlabel{sec:crossval}{{IV}{2}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  Results of the regression on the \texttt  {v0c0} dataset. We used 2 hidden layers with 100 neurons, $N_{\rm  batch}=64$, 250 epochs, constrained output. See Sec.\nobreakspace  {}III{}{}{}\hbox {} for discussion. The colors in the first plots is related to the absolute difference between predicted and injected. The black line is the bisector.}}{3}{}}
\newlabel{fig:v0c0_results}{{2}{3}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  Results of the regression on the \texttt  {v1c0} dataset. We used 2 hidden layers with 100 neurons, $N_{\rm  batch}=64$, 250 epochs, constrained output. See Sec.\nobreakspace  {}III{}{}{}\hbox {} for discussion. The colors in the first plots is related to the absolute difference between predicted and injected. The black line is the bisector.}}{4}{}}
\newlabel{fig:v1c0_results}{{3}{4}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  $R^2$ coefficients for different architectures. We show only the models that have an $R^2$ coefficient above the threshold indicated in the title of each panel. We show the results for the mean of $R^2$ and for the $R^2$ of each feature. In the rainbow-scatter plot the green circle marks the highest $R^2$ coefficient. }}{5}{}}
\newlabel{fig:crossval_v0c0}{{4}{5}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces  Test-dataset, compare with Fig.1 of Chatterjee-1911.00116. Do the systematics in the waveform model matter?}}{6}{}}
\newlabel{fig:GstLAL_paperlike}{{5}{6}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {V}GstLAL BNS dataset}{6}{}}
\newlabel{sec:GstLAL}{{V}{6}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Loss function}{6}{}}
\newlabel{sec:loss}{{VI}{6}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {VII}GstLAL BNS dataset - a different approach}{6}{}}
\newlabel{sec:GstLAL2}{{VII}{6}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces  Results of the regression on the \texttt  {GstLAL} dataset. We used one hidden layer with 100 neurons, $N_{\rm  batch}=64$, 50 epochs, unconstrained output. The colors in the first plots is related to the absolute difference between predicted and injected. The black line is the bisector. The final $R^2$ coefficients for $m_1$, $m_2$ and $M_c$ are 0.7363, 0.7819, and 0.9999, respectively. In the last three panels we plot $m_2$, $M_c$ and $q$ over $m_1$ for the injected, recovered and predicted quantities.}}{7}{}}
\newlabel{fig:GstLAL_results}{{6}{7}{}{}{}}
\bibdata{regression_NNtfNotes,refs,local}
\bibstyle{apsrev4-1}
\citation{REVTEX41Control}
\citation{apsrev41Control}
\newlabel{eq:m12frompMc}{{9}{8}{}{}{}}
\newlabel{eq:nu_condition}{{11}{8}{}{}{}}
\newlabel{eq:m12frompMc_mod}{{12}{8}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {VIII}Things to discuss}{8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces  Results of the regression on the \texttt  {GstLAL} dataset using the approach of Sec.\nobreakspace  {}VII{}{}{}\hbox {}. We used one hidden layer with 100 neurons, $N_{\rm  batch}=128$, 100 epochs, unconstrained output. The colors in the first plots is related to the absolute difference between predicted and injected. The black line is the bisector. The final $R^2$ coefficients for $p=m_1 m_2$ and $M_c$ are 0.99993 and 0.99997, respectively. In the three panels in the middle we plot $m_2$, $M_c$ and $q$ over $m_1$ for the injected, recovered and predicted quantities. Finally, in the first two panels of the last row we show the (indirectly) predicted masses with the ($p$, $M_c$) regression versus the injected masses, while in the last two panels of the last row we show the predicted masses obtained with the standard regression of Sec.\nobreakspace  {}V{}{}{}\hbox {} versus the injected ones.}}{9}{}}
\newlabel{fig:GstLAL2_results}{{7}{9}{}{}{}}
\newlabel{LastPage}{{}{9}{}{}{}}
